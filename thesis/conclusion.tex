\chapter{Summary \& Conclusion}
\label{chap:conclusion}

In this dissertation, we posited the main hypothesis that inclusion of source-side context information, without
linguistically informed features, benefits translation quality. We have assessed this question from various different
angles. We set out to answer the following inter-related questions and have indeed largely already done so in
Section~\ref{sec:conclusion} of Chapter~\ref{chap:sourcecontextinsmt}:

\begin{enumerate}
\item To what extent can we improve translation by considering source-side
    context information?
\item What context features prove most effective?
\item To what extent are linguistically uninformed features effective?
\item How can source-side classifiers be used in translation tasks?
\end{enumerate}

Chapter~\ref{chap:clwsd} demonstrated an application in cross-lingual word sense
disambiguation, where word expert classifiers succesfully tackled the problem
of translating a word in context. Our system emerged as the winning system in
the SemEval 2010 Cross Lingual Word Sense Disambiguation task, and achieved
either winning or high scores in the same task in 2012. This second time
around, we placed focus on hyperparameter optimisation of the classifier
parameters and automatic feature selection per classifier expert, which
supersedes the voting approach we used the first time. Automatic feature
selection indeed leads to a modest improvement, whereas classifier parameter
optimisation does not.

At this stage, we experimented with some basic linguistically informed
features as well, i.e. part-of-speech tags and lemmas. Lemmas proved to be
beneficial indeed, whereas part-of-speech tags failed to make a positive
impact. Nevertheless, our study's explicit focus is to assess the efficacy of
the surface features in their pure form, i.e. not enriched with any linguistic
information. We employed multiple classifiers or word experts in which the
feature vector consists of a simple local context window. In the cross-lingual
WSD task our linguistically-uninformed classifiers easily surpass the
non-context-informed baselines. In other words, for this task, we find evidence that
corroborates our main hypothesis and gives positive support to our first and third question.

We extensively experimented with global context features at this stage, as
opposed to local context features found in the immediate neighbourhood of the
phrase to be translated. We found a discrepancy in results between the SemEval
2010 and 2012 runs. Subsequent attempts in later chapters support the SemEval
2012 findings, in which global context features do not lead to an improvement
in translation quality.

The initial successes with translation aided by source-side context information
led us to propose this technique for a novel application in computer-aided translation
assistance. In this task, we translate L1 fragments in an L2 context.
In Chapter~\ref{chap:colibritapilot} we tested the feasibility of this idea and
found our context-sensitive classifiers improve both upon a non-context informed
baseline, as well a baseline enriched with a language model. We then
proposed our task for SemEval 2014 in Chapter~\ref{chap:semeval2014task5}, and attracted
participation by six participants, some of whom achieve high results. It
becomes apparent, specifically, that phrase-based statistical machine translation is able to
achieve better results than pure classifier-based approaches, despite the
localised and cross-lingual nature of the task.

We confirm this in Chapter~\ref{chap:colibritafinal}, where we conducted an
extensive comparison of the pure classifier-based and SMT-based solutions to
our L2 Translation Assistance task. We then proceed to integrate classifiers in
the SMT-based solution, in an attempt to gain the best of both worlds. However,
we find that this does not lead to improved translation quality. Nevertheless,
our hypothesis that translation can be improved by considering source-side
context information is still upheld here.

In Chapter~\ref{chap:sourcecontextinsmt} we moved away from the translation of
mere localised fragments in a context, and experimented with full Statistical
Machine Translation, i.e. translation of whole sentences. We incorporated
source-side context information using classifiers in the SMT decoder, using a
bypass method. We tested various forms of integration, but found that we could
only really improve upon the baseline for one corpus (IWSLT 2006, Chinese to
English). All other results turned out on or below baseline, or failed to pass
rigourous significance tests. Our main hypothesis therefore does not hold for
full statistical machine translation. We ask ourselves why this is the case and
suggest that that the information we attempt to explicitly model in the
translation model, is already implicitly and satisfactorily covered by both the
target-side language model as well as the translation model. Integration of
linguistically-uninformed classifiers in statistical machine translation
therefore has no added value.

Our efforts only prove fruitful when used standalone or for localised problems,
whereas statistical machine translation proves to be a superior technique,
leading to higher translation quality, for those translation tasks that it can
tackle.

When it comes to classifiers and their integration in SMT, future research could continue in the direction that
past studies such as \cite{Rejwanul+11} have already proceeded; that of incorporating linguistic
features. The technical foundation for such studies is well prepared in this one: The integration and score weighting methods from Chapters~\ref{chap:colibritafinal} and especially
\ref{chap:sourcecontextinsmt} lay a foundation upon which expansions with specific linguistic features would make this a
feasible option. We do not think, however, that this is a promising line of continued investigation as the
novelty of such further studies would be limited, depending on the type of linguistic features
investigated and the way they are incorporated. Moreover, results from participants of our SemEval task, show
that intuitively promising linguistically-informed features, such as dependency features, do not always lead to the
expected improvement \citep{UNAL,IUCL}.

We especially have to concede that the state-of-the-art in Machine Translation has changed considerably during and after
the years this research has taken place; techniques such as Neural Machine Translation have appeared on stage and often
prove superior to the SMT paradigm we employed. What the possibilities of source-side context in Neural Machine
Translation are, is already an active subject of research \citep{Jean+17,Wang+17,Bawden+17,Maruf+17}. These studies
do mostly focus on wider discourse context than we did.

The academic merit of our study is primarily in deepening and closing a line of investigation; that of modelling
source-side context information, mostly through memory-based classifiers, and integration thereof in Statistical Machine
Translation. We have dived to considerable depth, beyond that of prior studies, to
squeeze whatever we could out of these techniques at our disposition, and to study their interplay in various settings.
At the same time, we subjected our experiments to high standards by testing on a wide variety of corpora, language pairs and incorporating necessary statistical significance tests.

Aside from the answers to the research questions provided by this dissertation, other important contributions of this
PhD project to the scientific community and future studies are 1) the Colibri Core software that was developed and
described in Chapter~\ref{chap:coco}, application of which goes well beyond what it was used for in this research; and
2) the cross-lingual translation assistance task we proposed in Chapter~\ref{chap:semeval2014task5} and solutions to
which were extensively studied.



