\pgfplotsset{compat=newest}
%\usetikzlibrary{external}
%\tikzexternalize



\chapter{Uninformed source-side context in Statistical Machine Translation}
\title{Uninformed source-side context in Statistical Machine Translation}
\label{chap:contextinsmt}
\label{chap:sourcecontextinsmt}

%\title{Language-independent classifier-based modelling of source-side context information in Statistical Machine Translation}



In this chapter we present a series of experiments focusing on the modelling of
source-side context to improve phrase-based Statistical Machine Translation. We
attempt to independently reproduce a line of existing research and test whether
considering source-side context information directly in the translation model
has a positive effect on translation quality.  We furthermore investigate
various ways in which discriminative classifier-based models can be integrated
into Statical Machine Translation.  We use proven techniques from Word Sense
Disambiguation, effectively integrating these techniques in Statistical Machine
Translation. Our approach is language-independent and knowledge-poor or
uninformed: we do not employ any explicit linguistic features computed by
part-of-speech taggers, word sense disambiguation systems, supertaggers, or
parsers, as used by previous work. We observe improvements in translation
quality by our method, but only clearly so for domain-specific corpora with a
high degree of formulaic language. Lowering the expectations raised by previous
work, we conclude that the explicit modeling of source-side context information
does not generally add much information to that already implicitly available in
the SMT decoding process.  


\section{Introduction}

In phrase-based Statistical Machine Translation (SMT) the problem of
translating an input sentence from a source language to a target language is
implemented as a game of probabilities and a search for the most probable
translation option.  These probabilities are expressed in a number of models
that specialise in a certain aspect relevant to the translation process. The
method is called ``phrase-based'' because phrases are the building blocks of
the translation model, generalising earlier approaches in
Statistical Machine Translation based on words.  Phrases in this
sense are sequences of one or more words, i.e. $n$-grams of variable
length (including unigrams). They are not constrained to form a
proper linguistic constituent of any kind.

We introduced phrase-based SMT in Chapter~\ref{chap:intro},
Section~\ref{sec:intromt}. To recap, two models are at the core of
phrase-based SMT: first there is the translation model which maps the
translation phrases in the source language ($S$) to phrases in target language
($T$). Each mapping between phrases is associated with a vector of
probabilities, $p({phrase}_S|{phrase}_T)$ and $p({phrase}_T|{phrase}_S)$. They
can be seen to model the notion of \emph{``semantic faithfulness''}; if a
phrase is translated from one language to another, the meaning should be
preserved as accurately as possible.

The second core model is the language model. This model is monolingual
in nature and models the target language. It models what words are
likely to follow others and can be interpreted as modelling the
\emph{``syntactic fluency''} notion of translation; a translation should be
in a natural word order and be a typical sequence of words for the
target language.

A Machine Translation \emph{decoder} optimises a log-linear model of that
combined these two, and optionally other models as well. Given an input
sentence in the source language, it searches through a vast space of all
``possible'' translation options, most nonsensical, for a path maximising the
probabilities according to each of the models, taking into account different
weights they may be assigned.

This chapter focusses on the role of additional
surface-form source-language context information in the SMT process. The Language Model models context
for the target language. Yet in SMT there is no component modelling
context for the source language, whereas intuitively source-side context may
provide a powerful cue for translation. Consider the word ``bank'' and its
Spanish translation in examples~\ref{ex:bank1} and \ref{ex:bank2}.

\begin{exe} %gb4e package
\ex \textbf{English:} I don't trust the bank. \\
    \textbf{Spanish:} No me fio del banco.
\label{ex:bank1}

\ex \textbf{English:} The boat headed towards the bank of the river. \\
    \textbf{Spanish:} El barco se dirigió hacia la orilla del río.
\label{ex:bank2}
\end{exe}

The same English word, ``bank'', may express multiple lexical semantic senses, some of
which are expressed by different words in Spanish. Source-side context
information may provide valuable clues to what sense is being intended, and
therefore what translation is correct.  The words ``boat'' and the phrase ``of
the river'' in example \ref{ex:bank2} provide clues that we are using
bank in its aquatic sense. Example \ref{ex:bank1} is less obvious, but the
word ``trust'' could be seen to be a clue for ``bank'' denoting a
financial institution.

We hypothesise that the inclusion of source-side context information in the
translation model of a phrased-based SMT system improves translation results,
as the context helps in providing a more accurate disambiguation. A
counter-hypothesis to this would be that although source-side context
information is not modelled explicitly in this paradigm, it is implicitly captured by the
combination of translation model and target language model, and explicit
modelling has no added value. 

There is an obvious overlap between what we aim to do here and the field of
Word Sense Disambiguation (WSD). We effectively test an integration of proven
techniques from WSD in Statistical Machine Translation, and apply these to
phrases rather than just words.

WSD systems often employ a variety of linguistic features. The focus of our
study, however, is to be independent of the explicit computation of
linguistically abstract features with language-dependent tools such as
lemmatisers, part-of-speech taggers, supertaggers, or dependency parsers.
Integration of such techniques has already been researched by \cite{Rejwanul+11}. 
%We are interested only in the surface
%forms, the text as-is, and stay as close as possible to vanilla Phrase-Based
%Statistical Machine Translation, without using any language-specific external
%resources. 
We attempt to assess the merit of source-side context information as purely as possible.

Not introducing extra data for the translation system means our goals have to
be set more modest as well. We may not expect the same gains as are achieved by
introducing extra data from linguistic preprocessors.

In the next section, we will sketch an overview of previous research.
Section~\ref{sec:methodology} describes how our system is set up. In
Section~\ref{sec:experiments} we introduce the data sets used and the
experiments we conducted. The results are presented, analysed and
discussed in Section~\ref{sec:results}. In Section~\ref{sec:conclusion} we come
to the final conclusion.

\section{Previous research}

The idea to integrate WSD approaches in SMT is not new, nor is the idea to use
source-side context information to disambiguate in translation. Various studies
have been conducted, offering mixed results. In the early days of SMT,
\cite{GarciaVarea+02} explicitly modelled source-side context in a
maximum entropy model for word-based SMT, and report slightly improved error
rates on a translation task.

\cite{CarpuatWu05} were the first to tackle the question whether full-scale WSD
models were beneficial to translation quality when integrated in SMT systems,
and thus their work forms an important foundation for our own study.  Their
approach uses an ensemble classification model that integrates
position-sensitive, syntactic, and local collocational features, a combination
with proven merit in competitive WSD tasks. This includes linguistic features
such as part-of-speech tags and lemmas, as well as more complex syntactic
relations.  They test on a single Chinese-to-English test set only, and only
evaluate with BLEU, which leaves open the question whether their conclusions
would hold on different language pairs, test sets, and using different
evaluation metrics.  Their approach is word-based and the level of
integration is also limited.

Carpuat and Wu focus on the WSD model rather than on the SMT
model, whereas we place more focus on the SMT model and the
integration method, and keep our WSD model relatively
simple.

Despite their efforts, they reach the surprising conclusion that inclusion of
WSD techniques does \emph{not} yield better translation quality. Will these
results hold in a more modern phrase-based Statistical Machine Translation
approach? Indeed, two years later they expanded their study to full phrasal units
\citep{CarpuatWu07} and, for the first time, found results that did support the
hypothesis that SMT benefits from the integration of WSD techniques. They now
focus on better integration in \emph{phrase-based} SMT: ``Rather than using a
generic SenseEval model as we did in \cite{CarpuatWu05}, here both the WSD
training and the WSD predictions are integrated into the phrase-based SMT
framework.'' \citep{CarpuatWu07}. They also broaden their use of evaluation
metrics, yet still test on only Chinese to English.

The work of \cite{Gimenez+07}, from the same year, follows a similar
strategy. They use support vector machines to predict the phrase
translation probabilities for the phrase-translation table component
of SMT, rather than relying on the context-unaware Maximum Likehood
Estimate the statistical process produces. The feature vector for
their classifiers consists of both local context as well as global
context features.  In addition to the surface forms of the words, they
rely on shallow linguistic features such as part-of-speech tags and
lemmas. They conduct a manual evaluation judging fluency and
adequacy, and conclude that considering context improves semantic
adequacy, yet does not benefit syntactic fluency. They remark that the
integration of the classifier probabilities in an SMT framework needs
further research, which is something that will indeed be a focus in
our present study.

The year 2007 saw a culmination of various studies integrating WSD techniques
in SMT using classifiers. A third study in this trend was \cite{Stroppa+07}.
They focus on the word form, as does this present study, and add only
part-of-speech features. On a dataset from the IWSLT 2006 challenge, for
Chinese--English and Italian--English, they achieve a significant improvement
for the former language pair, whereas the BLEU score for the latter language
pair fails to pass the significance test. We attempt to reproduce the
Chinese--English experiment in this study.

Source-context aware translation has also been attempted outside of the
predominant SMT framework. For instance, \cite{MBMT} implement a
form of example-based machine translation that is word-based and relies
chiefly on classifiers for the translation model component. Two studies derive
from the same concept while transcending a word-based paradigm:
\cite{MARKERBASED} use chunks delimited by common markers, and \cite{PBMBMT}
attempt a full extension to phrases similar to SMT. Although positive results
are achieved in the latter study, it does not rival state-of-the-art SMT.

The most complete study we build upon is \cite{Rejwanul+11}, which in turn
draws from the majority of the aforementioned studies, and provides an
extensive comparison between them. Their study finds that including
linguistically-informed contextual features produces improvements, but not
always; also, different contextual features have different, unpredictable
results, some of which positive. The main contrast between our study and theirs
is that Haque et al focus on a variety of linguistically-informed contextual
features, whereas we depart from a language-independent angle, i.e. we do not
presuppose the presence of natural language processing tools for computing
linguistically abstract features. Furthermore, we intend to settle the
conflicting reports on whether this may lead to an improvement in translation
quality. A notable focus in our study will be possible methods of integrating
the classifier probabilities in the SMT, as recommended also by
\cite{Gimenez+07}. Furthermore, we take into account and compensate for the
instability of parameter optimisation techniques.


\section{Methodology}
\label{sec:methodology}

Like most of the previous work, we approach the machine translation
problem in a phrase-based fashion, which has superseded the simpler word-based
paradigm for quite some time. This means that phrases, defined as a
sequence of one or more words (that need not form a linguistic entity in any
way), form the basic building blocks of our translation model. The problem of
translating a sentence is decomposed into the problem of translating its phrasal
sub-parts and combining the results in the best order.

In describing our methodology, we first focus on the problem of phrasal
translation, adding in the source-side context component. This shall be done
using discriminative classifiers. Then we address how this can be integrated into a
phrase-based Statistical Machine Translation decoder, which takes care of
the ordering aspect. 


\subsection{Modelling source-side context with classifiers}

In line with several previous studies \citep{Rejwanul+11,PBMBMT,
  Stroppa+07,MARKERBASED}, we make use of memory-based classifiers to
build a translation model informed by source-side context
information. More specifically, we will be using IB1 \citep{IB1}, an
implementation of $k$-Nearest Neighbour classification; IGTree
\citep{IGTree}, an optimised and lossless tree-based approximation
thereof; and TRIBL2, a mixture of IB1 and IGTree.

These algorithms are all implemented in the software package TiMBL
\citep{TIMBL}\footnote{\url{http://languagemachines.github.io/timbl}, version 6.4.5 is the
version we used.} and are well-suited for symbolic data and highly multivariate
class spaces.  Moreover, memory-based classification has been a successful
method in the field of Word Sense Disambiguation \citep{SENSEVAL2}, and as
demonstrated as well in Chapter~\ref{chap:clwsd}.

When speaking of the $k$ nearest neighbours in the implementation of IB1,
IGTree and TRIBL2; we are actually referring to the neighbours at nearest
distance $k$. So even with $k=1$ we may be talking about multiple data points
that are all at equal distance.

%(these two paragraphs are paraphrased from my PBMBMT thesis, not sure to cite
%or prevent for risk of over-self-citation here) % [Antal] that's no problem.
IGTree compresses the instance base into an ordered decision tree structure at
training time, and issues look-ups in this tree structure at test time. Unlike
other top-down induced decision tree methods such as C4.5, features in IGTree
are tested in a fixed order, computed only once for all features. This order is
determined using metrics such as \emph{information gain} or \emph{gain
ratio}\footnote{This is the default that is also used in our experiments}. They
determine the relative informativeness or disambiguating power of each feature
and provide a ranking of all features. 

IGTree's performance relies on the differences in gain ratio between
features. If these are small then IGTree may perform significantly below IB1
\citep{TIMBL}. A hybrid approach called TRIBL2 \citep{TIMBL} starts out with
IGTree and switches to a IB1 variant when a value mismatch occurs in the tree.
In this study, we therefore opt to use TRIBL2 over plain IGTree, but only when
using IB1 would have a prohibitively large impact on performance.

In our classifier-based translation model we model the probability
of a target phrase ($t$) given a source phrase ($s$) and context information
($C$). We can express this as $p(t|s,C)$.  \cite{Stroppa+07} state that
direct estimation of this probability using relative frequencies would result
in overestimation of large phrases, and that therefore a smoothing factor is
required. They proceed to say that through memory-based classification they
introduce precisely such a smoothing factor implicitly.

Given a source phrase and context information, the classifier yields classes
corresponding to target phrases, with an associated weight. After
normalization, these can be considered a posterior distribution of
target phrases. 

We focus on the modelling of local context, i.e. words in the
immediate vicinity of the source phrase. Take $w_0$ to be the first word of
the source phrase $s$, then for a local context size of $n$ words to the left and
$m$ to the right we construct the feature vector $C$ as
follows\footnote{For context words out of the sentence's bounds, placeholders
are used instead}:

\begin{equation}
  C = \langle w_{-n} .. w_{-1} , s , w_{|s|+1} .. w_{|s|+m} \rangle
\end{equation}

Now there are two ways in which we can construct a classifier:

\begin{itemize}
  \item \textbf{Monolithic classifier} -- One aggregated classifier for all
    source phrases. Used with the TRIBL2 algorithm.
  \item \textbf{Classifier experts} -- One classifier per source phrase. Used
      with the IB1 algorithm.
\end{itemize}

In this study we will use and compare both methods, which is, for the task at
hand, the first such a comparison in the literature as far as we know.

For the monolithic classifier, the first feature in the ranking will always be
$s$. Nevertheless, it is quite conceivable that a match for the context is not
found and the classifier proceeds to match on another feature. To prevent
situations in which the classifier falls back to a completely different source
phrase, and thus comes up with unrelated translation options, we enforce that
the source phrases need to match, which is what \cite{Stroppa+07} do as
well.

For the classifier experts, on the other hand, the source
phrase carries no discriminative power, as it is shared amongst all instances. We
therefore omit it from the feature vector.

\subsection{Training}

The translation model is trained on parallel corpus data. We follow a common MT
pipeline, and additionally derive classifier training data from the
resulting phrase-translation table and the parallel corpus.

Given a tokenised and sentence-aligned parallel corpus, we iteratively learn
word alignments using GIZA++ \citep{GIZA}. Then we identify and extract phrase
pairs using the {\em grow-diag-final}\/ algorithm \citep{OchNey2003}. The
result is a phrase-translation table mapping source phrases to target
phrases, along with associated scores which we will discuss in the next
section. %This phrase-translation table effectively constitutes the translation
%model.

The translation model would be finished if we would want to leave it to be
non-context-informed. We have some additional steps to perform to train our
context-informed classifiers. We take the phrase-translation table, along with
the parallel corpus, as a basis for extracting training instances.

We build indexed pattern models of all source phrases and target phrases that
occur on their respective side of the parallel corpus, and which also occur in
the phrase-translation table. An indexed pattern model maps each distinct
phrase to the locations in the corpus where it occurs.  Additionally, a reverse
index is included in the model for the target-side of the corpus, which maps
any given $(sentence, token)$ position to a set of phrases that begins at that
position.  This is computed using the software package
\emph{colibri-core},\footnote{\url{http://proycon.github.io/colibri-core},
release v2.1.2 fair state of the software version during this study, although
some experiments were conducted with older versions}, as introduced in
Chapter~\ref{chap:coco},  which takes care of a losslessly compressed in-memory
representation for all phrases, and allows us to cope with large corpora. Given
these two pattern models $M_{source}$ and $M_{target}$ we can extract the
context for each phrase pair quickly and efficiently, as shown in simplified
form in Algorithm~\ref{alg:featureextract}.

\begin{algorithm}
\caption{Algorithm for feature extraction for training classifiers.  Take $n$
again to be the left context, $m$ to be the size of the right context, and
$w{(i,j)}$ to denote the word in the source corpus in sentence $i$, token $j$.
The vector $C$ represents the context information and constitutes the feature
vector.  The algorithm will return a list containing two-tuples $(C,t)$.  }
\label{alg:featureextract}
\begin{algorithmic}
\State instances $\gets []$
\For {$(s \in M_{\text{source}}, t \in M_{\text{target}})$}
  \For {$i \in (M_{\text{source}}[s] \cap M_{\text{target}}[t])$}
    \For{$j \in M_{\text{source}}[s][i]$}
        \State $C \gets \langle w_{i,j-n} \ldots w_{i,j-1}, s, w_{i,j+|s|+1} \ldots w_{i,j+|s|+m} \rangle$
        \State \Call{instances.append}{$(C, t)$} 
      \EndFor
  \EndFor
\EndFor \\
\Return{instances}
\end{algorithmic}
\end{algorithm}
    
\noindent
This algorithm is implemented in \emph{colibri-mt}.
\footnote{\url{https://github.com/proycon/colibri-mt} \\ version v0.2
represents a fair state of the software version during this study.}

The returned instances can be stored directly, either in a single model for the
monolithic approach or in separate models for each $s$ for the classifier
expert approach. A final training phase then computes the feature ranking and
transforms this data into the instance base format required for TiMBL.

%When extra training data for the classifier(s), 
It may well happen that either 1) an $(s,t)$ pair only occurs once, or
2) a pattern $s$ occurs in multiple contexts but all map to the same
$t$. In such cases, a context-informed classifier has no added value
and therefore such instances are omitted from the training data.

\subsection{Integration in an SMT Decoder}
\label{sec:smtintegration}

The task of an SMT decoder is to find the best translation among a pool
of possible translation hypotheses. The best translation hypothesis is the
translation hypothesis that maximises a log-linear combination and is sought
after in a beam-search algorithm. This log-linear combination draws from
various models, such as a translation model (i.e. the phrase-translation
table), a target-language model, and optionally additional models such as a
distortion model and a word-reordering model. 

The SMT model is generally expressed as in Equation~\ref{eq:smtmodel}, taking
sentence $e$ to be the translation in the target language, and $f$ to be the sentence to be
translated, in the source language.

\begin{equation}
\argmax_e p(e|f) = \argmax_e p(f|e)p(e)
\label{eq:smtmodel}
\end{equation}

Bayes' rule inverts the problem into two factors\footnote{The normalisator
denominator p(f) can be dropped as it would not alter the outcome of the
\emph{argmax} function}, the former corresponding to the translation model, and
the latter corresponding to the language model. 

The translation model is a mapping of the set of source phrases ($S$) to the
set of target phrases ($T$). Each phrase pair $(s,t)$ where $s \in S$ and $t
\in T$ is described by a score vector indicating the likelihood of
translation. This score vector most notably consists of the probabilities
$p(s|t)$ and $p(t|s)$. In addition, the lexical weighting probabilities $lex(s|t)$
and $lex(t|s)$ express the probability of a phrase pair word-by-word, and are
often included as components in the score vector. During decoding, the total
score of the translation model and other models is expressed as a log-linear
combination, in which different weights can be assigned to each of the
components of the score vector. These weights are hyperparameters to the task and
are typically \emph{optimised} automatically on development data using for instance
Minimum Error Rate Reduction (MERT) \citep{MERT}.

The probability $p(t|s)$ is the one we are interested in. Recall that the
classifiers seek to model $p(t|s,C)$, where $C$ constitutes the vector of
context information. The hypothesis under investigation in the present study is
that $p(t|s,C)$ is a more accurate measure than $p(t|s)$.

The state-of-the-art SMT decoder used in the majority of MT studies is Moses
\footnote{\url{http://statmt.org/moses/}, \\ git commit
\texttt{1615d56b69df5d959de9c5416cf628d76ac99169} represents a fair state of the
version used in this study.} \citep{MOSES}. Moses offers no facilities to take
source-side context information in account. We considered three options to
achieve our goal of integrating source-side context: 1) creating a new decoder;
2) enhancing Moses; or 3) using a bypass method. Although we initially set out
to create a new decoder, it proved to be too time-consuming to attain the same
quality as Moses.  We therefore decided, in line also with most of the
literature, to follow the third option and use a bypass method; this allows us
to use Moses as a black box.

The bypass method is a \emph{discriminative translation filtering}
step \citep{Rejwanul+11}. It performs context-aware classification in
a pre-processing step, namely through alteration of the
phrase-translation table, and bypasses the need to alter the
decoder. Taking each sentence individually, we ensure that the entries
in the phrase-translation table are explicitly tuned to the
source-side context. The output of the classifier(s) acts as the
filter and constrains the translations options, as well as adjusts the
score vector. Each instance of a source phrase receives a separate entry
in the phrase-translation table, as opposed to one source phrase
applying to all instances in the test data.

To achieve this, each source phrase in the phrase-translation table is
replaced by a representation of its position in the test data, e.g.
$(1,0)$ for first sentence, first word.  This is done using the
software package \emph{colibri-mt}. It creates an indexed pattern
model on the test data, constrained by the phrases in the
phrase-translation table. This thus constitutes a mapping of all
distinct source phrases in the phrase-translation table to the indices
in the test corpus that are instances of these phrases.

Subsequently we invoke the classifier(s) and construct the altered
phrase-translation table as shown in Algorithm~\ref{alg:contextmoses}.

\begin{algorithm}
\begin{algorithmic}
  \For {$s \in M_{\text{test}}$}
  \For {$(i,j) \in M_{\text{test}}[s]$}
    \State $C \gets \langle w_{i,j-n} \ldots w_{i,j-1}, s, w_{i,j+|s|+1} \ldots w_{i,j+|s|+m} \rangle$
    \State $[(t, p(t|s,C) )] \gets$ \Call{classify}{$s,i,j,C$}
    \State {\Call {appendtophrasetable}{$s,i,j,[(t, p(t|s,C)]$}}
  \EndFor
\EndFor
\end{algorithmic}
\caption{Classifier invocation on test data. Take $M{\footnotesize\text{test}}$ to be the pattern
model of the test data, i.e. a map of source phrases occuring in the test
data, and $[(t,p(t|s,C))]$ to be a list of translation options ($t$) with
associated probability $p(t|s,C)$.}
\label{alg:contextmoses}
\end{algorithm}

In Algorithm~\ref{alg:contextmoses} we examine each source phrase in turn, find
where it occurs in the test data ($i,j$) using the pattern model
($M_{\text{test}}$), and extract context information ($C$). The context
information constitutes our feature vector, with which we invoke the
appropriate classifier. This is either the monolithic classifier, or the
classifier expert pertaining to the source phrase under consideration. The
result of this classification is a distribution of translation options for that
source phrase in the given context, along with a classifier score for each
option. After normalisation, this score is $p(t|s,C)$. We now have two score
weighting methods for integrating this in the score vector for the phrase pair:

\begin{itemize}
  \item \textbf{Replace} - Replace the $p(t|s)$ probability with $p(t|s,C)$;
  \item \textbf{Append} - Leave the existing $p(t|s)$ as it was, and append
    $p(t|s,C)$ as a new score to the score vector.
\end{itemize}

The score weighting of choice is applied and the data is entered into the
altered phrase-translation table. The source phrase takes the form of a
sequence of $(i,j)$ indices, rather than the actual words. The test data is
replaced with a series of consecutive positions as well. These two altered data
sets are now the input to Moses, which can now run unmodified. 

\section{Experiments}
\label{sec:experiments}

We conduct a series of experiments to assess whether integration of
context-informed classifiers in Statistical Machine Translation leads to
an improvement in translation quality. 


\subsection{Data sets}

For the translation model, we rely on a parallel corpus as our main source of
input. Whenever the source is untokenised, we tokenise the data using
\emph{ucto}.\footnote{An open-source regular-expression based tokeniser with
unicode support, \url{http://languagemachines.github.io/ucto}, we used version 0.7.0} For the language model, we
simply reuse the target-side portion of the parallel corpus; we do not
introduce additional data for the language model in our experiments.

We use various corpora\footnote{EMEA and JRC-Acquis can be obtained through
\url{http://opus.lingfil.uu.se/}} and test various language pairs as we hope to
come to a general conclusion. Table~\ref{tab:datasets} lists them all, along with the amount of
sentence pairs we used for training, development and testing.

\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lrr}
\hline
 & \textbf{Languages} & \textbf{Sentence Pairs} \\
\hline
%\multicolumn{3}{l}{\pbox{12cm}{\textbf{Europarl} -- \emph{The proceedings of the
%European Parliament} \citep{EUROPARL,OPUS2012}} } \\
%& English to Spanish & $250,000$ / $2000$ / $2000$ \\
%& English to Dutch & $250,000$ / $5000$ / $5000$ \\
%\hline
\multicolumn{3}{l}{\textbf{EMEA} -- \emph{Documents of the European Medicines
Agency} \citep{OPUS2012} } \\
 & Spanish to English & $1,088,333$ / $5000$ / $5000$ \\
 & English to Dutch & $1,080,894$ / $5000$ / $5000$ \\
\hline
\multicolumn{3}{l}{\pbox{12cm}{\textbf{Fryske Akademy Parallel Corpus} -
\emph{A collection of texts in
Frisian and Dutch, contains numerous books and other sources} \citep{OERSETTER}
\footnote{This corpus is not publicly available unfortunately} } } \\
 & Dutch to Frisian & $137,937$ / $2000$ / $2000$ \\
 & Frisian to Dutch & $137,937$ / $2000$ / $2000$ \\
\hline
\multicolumn{3}{l}{\textbf{JRC-Acquis} - \emph{A collection of legislative documents of the
European Union} \citep{OPUS2012} } \\
 & English to Spanish & $1,233,670$ / $5000$ / $5000$ \\
 & English to Spanish & $250,000$ / $5000$ / $5000$ \\
\hline
\multicolumn{3}{l}{\pbox{12cm}{\textbf{IWLST 2012 TED Talks} - \emph{Transcripts and translations of TED
talks, used for subtitling, as used in the IWSLT 2012 Evaluation Campaign}
    \citep{WIT3,IWSLT12} } } \\
 & English to Dutch & $127,806$ / $885$ / $1569$ \\
\hline
\multicolumn{3}{l}{\pbox{12cm}{\textbf{IWLST 2006 Evaluation Campaign} -
\emph{Collection of phrases from a phrasebook / traveller's guide } } } \\
 & Chinese to English & $40,274$ / $489$ / $486$ \\
\hline
 \multicolumn{3}{l}{\pbox{12cm}{\textbf{Yandex 1M Web Corpus}  -
 \emph{Phrases crawled from the web by Russian search engine Yandex} } } \\
 & English to Russian & $990,000$ / $5000$ / $5000$ \\
\hline
\end{tabular}}
\caption{Overview of parallel corpora used for experiments. The three values
for sentence pairs correspond to the size of the training, development and test
set, respectively.}
\label{tab:datasets}
\end{table}

All our experimental end-results and all non-restricted data sets used can be
freely downloaded.\footnote{Download archive of all experimental data \& results:
\url{http://academictorrents.com/details/3cc11d076c62bfa0480afc187a4bb6e4759e5c16}}
%TODO: update with new results!!

\subsection{Evaluation}

We assess translation quality along three widely-used automated metrics,
as human evaluation is prohibitively expensive:

\begin{enumerate} %paraphrased from PBMBMT thesis
\item \textbf{BLEU} - BLEU \citep{BLEU} is probably the most widely-used metric
in Machine Translation. It computes a weighted average of $n$-gram overlap
between the system output and reference output. The score thus increases as
more $n$-grams in the reference translation are found in the system output.
%\item \textbf{NIST} - NIST intends to improve upon BLEU. It takes into account
how informative a particular $n$-gram is by assigning more weight to rare
$n$-grams and less to highly-frequent $n$-grams.  
\item \textbf{METEOR} - METEOR \citep{METEOR} also attempts to improve upon
  BLEU and attempts to emulate human judgement better. Unlike the prior
  metrics, it places emphasis on recall rather than precision.
\item \textbf{TER} - Translation Error Rate \cite{TER} aims to measure the
amount of editing that a human would have to perform to change a system output
so it exactly matches a reference translation. 
%\item \textbf{Word-Error Rate (WER)} - This is a simple metric derived from the
%minimum edit-distance, i.e. Levenshtein, algorithm. It counts the number of
%substitutions, insertions and deletions necessary to transform the translation
%into the reference sentence, with words as the basic unit. The lower
%the score, the more similar the translation is to the reference.  
%\item \textbf{Position Independent Word Error Rate (PER)} - This is a variant of the
%metric above, but here the order of the words is not taken into account. Any
%ordering of the same words will have the same score. The lower the score, the
%more similar the translation is to the reference.
\end{enumerate}

For BLEU and METEOR, higher scores are better, for TER, however, a lower score
corresponds with a better translation quality.

For each experiment we construct a non-context-informed baseline. This
baseline does make use of our full pipeline, i.e. the bypass method described
in Section~\ref{sec:smtintegration}, but it does not invoke the
classifiers.  We do this to ensure the only difference is the actual
integration of context information, and that differences in results can not be
attributed to minor idiosyncrasies of the implementation.

%old situation:
%We test statistical significance of our systems on the BLEU score, compared to
%the non-context informed baseline For this we use pairwise bootstrap sampling
%\citep{KoehnStatSig} with $5000$ iterations. Statistical significance at
%$p<0.05$ will be marked with an asterisk in the results.

\subsection{Parameter optimisation}
\label{sec:paropt}

An SMT system relies heavily on parameter optimisation. Each component of the
score vector is weighted by a separate parameter. These parameters are
experimentally determined on development data using Minimum Error Rate
Reduction \citep{MERT}, in which we optimise on the BLEU score. 

%old situation:
%MERT is,
%however, known to have high variance across multiple runs, due to many local
%optima in the search space. We therefore do not run MERT independently on each
%experimental run, but only on the baseline run, and use those weights for
%subsequent runs using the replace method.\footnote{The append method would
%require an extra weight that MERT would not have optimised.} Again, the
%motivation here is to keep all variables equal except for the integration of
%context information, so we can make a fair assessment.

MERT is, however, known to have high variance across multiple runs, due to many
local optima in the search space. We therefore can not simply run MERT once for
each experiment, as that gives no basis for determining whether our results can
be considered statistically significant. To remedy this issue, we have to
control for optimiser instability. We do so by applying the methodology of
\cite{MERTCONTROL}, which runs MERT multiple times, employs \emph{bootstrap
resampling} to compute standard deviations and \emph{approximate randomisation}
to compute p-values for the assessment of statistical significance. All this is
implemented in the software \emph{MultEval}\footnote{Available from
\url{https://github.com/jhclark/multeval}} by Jonathan Clark.

For each of the evaluation metrics, BLUE, METEOR and TER. MultEval reports
three numbers in parentheses, these are the following, from \cite{MERTCONTROL}:

\begin{enumerate}
\item $\overline{s}_{sel}$ - Variance due to \emph{test set selection},
computed using bootstrap resampling for each optimiser run. This metric reports
the average variance over all MERT runs. Higher values indicate that more
replications, a larger test test, is desireable to draw reliable inferences.
\item $s_{opt}$ - Variance due to \emph{optimiser instability}. Calculated
directly as the variance of aggregate metric score over all MERT runs. Higher
values are indicative of optimisation on development data generalising well to
previously unseen test data, i.e.\ it leads to overfitting. 
\item $p$, - The $p$-value s calculated through approximate randomisation. It roughly
expresses the probability of the difference between the system output and
baseline occurring due to chance. Approximate randomisation uses random
permutations to similulate chance occurrences. The quality of this measure depends on the number of separate
optimisation runs, and is conditioned on the test test. 
\end{enumerate}

In all our experiments, we use three seperate optimisation runs, to keep
complexity and experimental run-time to a minimum whilst still reaping the
benefits of multiple runs.

%talk about parameter optimisation in classifier
While we conduct parameter optimisation for the parameters of the SMT
decoder, we do not do so for the classifiers. Yet, these too could
be optimised. This is only feasible for the classifier experts; recall
that the monolithic classifier necessarily has extra constraints
imposed on it to prevent translations that do not occur in the
phrase-translation table. This has the disadvantage of making it hard
to run standard parameter optimisation algorithms.

Optimisation of classifier parameters comes with a number of challenges.
Ideally, it should not be independent of the optimisation of the decoder
parameters, and the classifier parameters should be evaluated on the
end result, i.e. the full sentential translations, according to one or more MT
metrics. This, however, is prohibitively expensive as it vastly increases the
parameter search space and thus the complexity of the problem. As a concession
for the sake of computability, classifier parameter optimisation can be considered
independently and assessed through simpler cross-validation or leave-one-out
methods (depending on the number of instances in the classifier expert).  It
still introduces a significant computational bottleneck.  
This has been extensively researched in Chapter~\ref{chap:clwsd}, Section~\ref{sec:wsd2} in
the context of cross-lingual Word Sense Disambiguation, rather than full
Machine Translation. There classifier parameter optimisation showed an
overall negative effect, most likely due to overfitting, especially when
combined with variable context selection. 

Optimisation can be taken a step further by also optimising the local context
size, i.e. the make up of the feature vector, on a by-classifier basis, as
opposed to using the same context size for all classifiers.  Variable context selection by itself
did make a small positive impact in a WSD context (See Section~\ref{sec:wsd2}).

Given the results of this prior study, and due to the inherent computational
complexity of different layers of optimisation (even under an independence
assumption) we choose to limit ourselves to just optimisation of the SMT
decoder parameters in this study. These parameters have the most immediate and
largest impact as they are directly optimised on an evaluation metric that is
used.  Note that although we forego on explicit variable context selection, an
implicit form is still there for the classifier experts; the feature weights
are computed on a by-classifier basis.

\section{Results}
\label{sec:results}

In this section we will present the results of our experiments.  Our largest
issue with the present MT literature on this subject is that we too often see
that hypotheses are tested only on one or two corpora, with just one or two
language-pairs, and sometimes without proper significance testing. We feel that
from such results, generic conclusions are drawn too quickly. To prevent this
pitfall, our experiments are distributed amongst various corpora and language
pairs, and are subject to rigid significance tests.

Section~\ref{sec:results1} will focus on our main hypothesis; does inclusion of
surface-form source-side context information lead to improved translation
quality?  We posit a number of hypotheses to determine in which cases
source-side-context might work. Section~\ref{sec:quantanal} conducts a
quantitative analysis to assess whether the classifiers make an impact even if
this does not reflect in the translation quality.

Our secondary hypotheses concern the ways in which classifiers can be
integrated in SMT. We investigate the impact of the classifier type and
weighting methods.  These are assessed in Sections~\ref{sec:typeopt} and
\ref{sec:weighting}, respectively. We conclude the result section with a
qualitative analysis in Section~\ref{sec:qualanal}.

\subsection{Source-side context vs. no context}
\label{sec:results1}

The primary objective of our study is to assess the role of source-side context
information. To this end we conducted a series of experiments on various
corpora and language pairs. 

In all tables in this section, the context-aware configurations are named
l$x$r$y$, in which $x$ and $y$ refer to the size (in words/tokens) of the left and right contexts
respectively.  Recall that the classifiers may be built in two distinct ways, a
single \emph{mono}lithic configuration (using \emph{tribl2}), or a classifier
\emph{expert} configuration (using \emph{ib1}).  Secondly, score weighting can
be done either through the \emph{replace}  or the \emph{append} method, as
shown in respectively the upper and lower parts of the tables. 

All experiments have been optimised with MERT on BLEU as described in
Section~\ref{sec:paropt}.  The score-triple in parentheses corresponds to
$(\overline{s}_{sel},s_{opt},p)$ and represents the measures discussed in
Section~\ref{sec:paropt}: test set selection variance, optimiser instability
variance, and p-value with respect to the baseline.

\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER}  \\ 
\hline
Baseline \footnotesize{(replace)} & 26.8 (1.4/0.6) & 29.8 (0.7/0.2/-) & 56.8 (1.5/1.2/-) \\ 
l1r1 \footnotesize{(mono/replace)} & 27.7 (1.4/1.1/p=0.01) & \textbf{30.2} (0.7/0.1/p=0.01) & \textbf{55.5} (1.5/1.6/p=0.0) \\ 
l2r1 \footnotesize{(mono/replace)} & 27.2 (1.4/0.3/p=0.15) & 29.9 (0.7/0.2/p=0.47) & 56.5 (1.5/0.6/p=0.56) \\ 
l2r2 \footnotesize{(mono/replace)} & \textbf{28.0} (1.5/0.4/p=0.0) & 30.0 (0.7/0.2/p=0.26) & 55.8 (1.5/0.6/p=0.02) \\ 
l3r3 \footnotesize{(mono/replace)} & 27.2 (1.4/0.3/p=0.28) & 29.8 (0.7/0.1/p=0.95) & 57.6 (1.5/0.4/p=0.06) \\ 
l1r1 \footnotesize{(experts/replace)} & 26.6 (1.4/0.2/p=0.58) & \textbf{30.2} (0.7/0.1/p=0.03) & 57.0 (1.5/0.1/p=0.57) \\ 
l2r1 \footnotesize{(experts/replace)} & 27.0 (1.4/0.8/p=0.5) & 30.0 (0.7/0.1/p=0.39) & 56.9 (1.5/0.8/p=0.71) \\ 
l2r2 \footnotesize{(experts/replace)}& 27.2 (1.4/0.3/p=0.21) & 30.1 (0.7/0.2/p=0.09) & 56.6 (1.5/0.9/p=0.7) \\ 
l3r3 \footnotesize{(experts/replace)}& 26.2 (1.4/0.5/p=0.2) & 29.8 (0.7/0.2/p=0.83) & 58.1 (1.5/0.8/p=0.0) \\ 
\hline
Baseline \footnotesize{(append)} & 26.8 (1.4/0.5/-) & 29.8 (0.7/0.1/-) & 57.2 (1.5/0.6/-) \\ 
l1r1 \footnotesize{(mono/append)} & 26.8 (1.4/0.3/p=0.92) & 30.0 (0.7/0.1/p=0.23) & 57.3 (1.5/0.5/p=0.84) \\ 
l2r1 \footnotesize{(mono/append)}& 27.6 (1.4/0.2/p=0.01) & 30.2 (0.7/0.0/p=0.04) & 55.9 (1.5/0.2/p=0.0) \\ 
l2r2 \footnotesize{(mono/append)}& 27.4 (1.4/0.8/p=0.09) & 29.7 (0.7/0.2/p=0.44) & 57.0 (1.5/0.9/p=0.62) \\ 
l3r3 \footnotesize{(mono/append)}& 27.1 (1.4/0.8/p=0.35) & 29.8 (0.7/0.2/p=0.91) & 57.2 (1.5/1.3/p=0.93) \\ 
l1r1 \footnotesize{(experts/append)} & 25.9 (1.4/0.5/p=0.02) & 30.0 (0.7/0.0/p=0.34) & 57.6 (1.5/0.7/p=0.37) \\ 
l2r1 \footnotesize{(experts/append)} & \textbf{27.9} (1.4/0.2/p=0.0) & \textbf{30.3} (0.7/0.2/p=0.01) & \textbf{55.6} (1.5/0.4/p=0.0) \\ 
l2r2 \footnotesize{(experts/append)} & 27.3 (1.4/0.5/p=0.15) & 30.0 (0.7/0.3/p=0.25) & 56.5 (1.5/0.4/p=0.05) \\ 
l3r3 \footnotesize{(experts/append)} & 26.3 (1.4/0.7/p=0.25) & 29.5 (0.7/0.1/p=0.14) & 58.7 (1.5/0.4/p=0.0) \\ 
\end{tabular}}
\caption{Results for Chinese to English on IWSLT 2006 Evaluation Campaign data.
The numbers in the context-aware configurations (l$x$r$y$), refer to the size
of the left respectively right context. }
%In parentheses is the triplet ($\overline{s}_{sel}$,$s_{opt}$,$p$).}
\label{tab:iwslt2006zhen}
\end{table}


In this section we focus exclusively on the
question whether context-informed systems are beneficial; a comparison of the
score weighting methods and classifier configurations is postponed until a
later section.  

The first results, on the IWSLT 2006 Evaluation Campaign, Chinese to English,
are listed in Table~\ref{tab:iwslt2006zhen}. A dataset from the same
source\footnote{This dataset is not publicly available. As we do not have the
exact set used by \cite{Stroppa+07}, we have to assume our training and
test sets, whilst drawn from the same source, are different subsets than the
ones used by \cite{Stroppa+07}.} was used in \cite{Stroppa+07}, who also
reported positive results.  We observe scores over the baselines for all
experiments.  However, not all pass the strict significance tests we impose.
Only configurations l1r1/monolithic/replace and l2r1/monolithic/append and
l2r2/experts/append pass the significance tests on all metrics, where we
consider a $p <= 0.05$ to be the passing criterium. Other configurations either
do not pass the significance tests at all, or paint a conflicting picture for
certain metrics. Nevertheless, we can conclude that in this batch of
experiments incorporating source-side context information using machine learning proves
beneficial for certain configurations.

In Table~\ref{tab:iwslt2012} we see results on the IWSLT 2012 corpus, for the
Dutch to English.  Here we observe that we do not manage to surpass the non-context informed baseline with
an acceptable degree of significance.

\begin{savenotes}
\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER}  \\ 
\hline 
Baseline \footnotesize{(replace)} & \textbf{31.3} (0.5/0.2/p=-) & \textbf{32.7} (0.2/0.1/p=-) & 47.9 (0.5/0.6/p=-) \\ 
%TODO: l1r1M is missing
l2r1 \footnotesize{(mono/replace)} & 30.8 (0.5/0.1/p=0.0) & 32.5 (0.2/0.0/p=0.0) & 48.1 (0.5/0.2/p=0.04) \\ 
l2r2 \footnotesize{(mono/replace)} & 30.5 (0.5/0.1/p=0.0) & 32.4 (0.2/0.0/p=0.0) & 48.5 (0.4/0.2/p=0.0) \\ 
l3r3 \footnotesize{(mono/replace)} & 30.4 (0.5/0.1/p=0.0) & 32.3 (0.2/0.1/p=0.0) & 48.4 (0.4/0.2/p=0.0) \\ 
l1r1 \footnotesize{(experts/replace)} & \textbf{31.3} (0.5/0.0/p=0.63) & 32.6 (0.2/0.0/p=0.05) & \textbf{47.1} (0.4/0.3/p=0.0) \\ 
l2r1 \footnotesize{(experts/replace)} & 30.9 (0.5/0.1/p=0.0) & 32.5 (0.2/0.0/p=0.0) & 48.3 (0.5/0.1/p=0.0) \\ 
l2r2 \footnotesize{(experts/replace)} & 31.0 (0.5/0.1/p=0.0) & 32.5 (0.2/0.1/p=0.0) & 47.9 (0.5/0.1/p=0.82) \\ 
l3r3 \footnotesize{(experts/replace)} & 30.8 (0.5/0.2/p=0.0) & 32.4 (0.2/0.1/p=0.0) & 48.2 (0.5/0.4/p=0.04) \\ 
\hline
Baseline \footnotesize{(append)} & 31.1 (0.5/0.3/p=-) & 32.6 (0.2/0.0/p=-) & 48.1 (0.5/0.7/p=-) \\ 
%TODO: l1r1M is missing
l2r1 \footnotesize{(mono/append)} & 30.8 (0.5/0.1/p=0.0) & 32.4 (0.2/0.0/p=0.0) & 47.4 (0.4/0.4/p=0.0) \\ 
l2r2 \footnotesize{(mono/append)} & 30.6 (0.5/0.4/p=0.0) & 32.4 (0.2/0.1/p=0.0) & 48.3 (0.4/1.1/p=0.12) \\ 
l3r3 \footnotesize{(mono/append)} & 30.5 (0.5/0.1/p=0.0) & 32.3 (0.2/0.0/p=0.0) & 48.5 (0.4/0.1/p=0.0) \\ 
l1r1 \footnotesize{(experts/append)} & \textbf{31.3} (0.5/0.1/p=0.11) & \textbf{32.6} (0.2/0.0/p=0.49) & \textbf{47.1} (0.4/0.5/p=0.0) \\ 
l2r1 \footnotesize{(experts/append)} & 30.9 (0.5/0.3/p=0.05) & 32.5 (0.2/0.0/p=0.0) & 47.9 (0.5/0.8/p=0.01) \\ 
l2r2 \footnotesize{(experts/append)} & 30.6 (0.5/0.3/p=0.0) & 32.4 (0.2/0.1/p=0.0) & 48.4 (0.5/0.8/p=0.01) \\ 
l3r3 \footnotesize{(experts/append)} & 30.7 (0.5/0.2/p=0.0) & 32.4 (0.2/0.0/p=0.0) & 48.3 (0.4/0.2/p=0.08) \\ 
\hline
\end{tabular}}
\caption{Results on IWSLT 2012, Dutch to English}
\label{tab:iwslt2012}
\end{table}
\end{savenotes}


We move on to larger corpora, such as the EMEA corpus and the JRC corpus, and
run experiments for English to Spanish and in the former corpus also for English
to Dutch. These results are shown in Table~\ref{tab:emea} for EMEA, and Table~\ref{tab:jrc} for JRC.

\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER} \\ 
\hline
\multicolumn{4}{|c|}{EMEA-en-es} \\
\hline
Baseline \footnotesize{(replace)} & 71.5 (0.5/0.0/p=-) & 80.9 (0.4/0.0/p=-) & 25.8 (0.6/0.0/p=-) \\ 
l1r1 \footnotesize{(mono/replace)} & 71.5 (0.5/0.0/p=0.89) & \textbf{81.0} (0.4/0.1/p=0.49) & \textbf{25.7} (0.6/0.1/p=0.02) \\ 
l2r1 \footnotesize{(mono/replace)} & 71.5 (0.5/0.0/p=0.43) & 80.9 (0.4/0.0/p=0.75) & 25.8 (0.6/0.1/p=0.78) \\ 
l2r2 \footnotesize{(mono/replace)} & 71.5 (0.5/0.1/p=0.65) & 80.9 (0.4/0.1/p=0.62) & 25.8 (0.6/0.1/p=0.44) \\ 
l3r3 \footnotesize{(mono/replace)} & 71.5 (0.5/0.0/p=0.72) & 80.9 (0.4/0.1/p=0.93) & 25.8 (0.6/0.1/p=0.32) \\ 
\hline
Baseline \footnotesize{(append)} & 71.5 (0.5/0.0/p=-) & 80.9 (0.4/0.0/p=-) & 25.8 (0.6/0.0/p=-) \\ 
l1r1 \footnotesize{(mono/append)} & 71.5 (0.5/0.0/p=0.74) & 80.9 (0.4/0.1/p=0.43) & 25.8 (0.6/0.1/p=0.92) \\ 
l2r1 \footnotesize{(mono/append)} & \textbf{71.6} (0.5/0.0/p=0.03) & \textbf{81.0} (0.4/0.0/p=0.42) & 25.8 (0.6/0.1/p=0.39) \\ 
l2r2 \footnotesize{(mono/append)} & 71.5 (0.5/0.1/p=0.4) & 80.9 (0.4/0.0/p=0.07) & 25.9 (0.6/0.1/p=0.17) \\ 
l3r3 \footnotesize{(mono/append)} & 71.5 (0.5/0.1/p=0.51) & 80.9 (0.4/0.1/p=0.74) & 25.8 (0.6/0.1/p=0.9) \\ 
\hline
\multicolumn{4}{|c|}{EMEA-en-nl} \\
\hline
Baseline  \footnotesize{(replace)}& \textbf{68.9} (0.6/0.1/p=-) & \textbf{0.0} (0.0/0.0/p=-) & 28.3 (0.7/0.1/p=-) \\ 
l1r1 \footnotesize{(mono/replace)} & \textbf{68.9} (0.6/0.0/p=0.83) & \textbf{0.0} (0.0/0.0/p=0.0) & \textbf{28.2} (0.7/0.0/p=0.04) \\ 
l2r1 \footnotesize{(mono/replace)} & 68.8 (0.6/0.0/p=0.28) & \textbf{0.0} (0.0/0.0/p=0.0) & 28.3 (0.7/0.1/p=0.2) \\ 
l2r2 \footnotesize{(mono/replace)} & 68.8 (0.6/0.0/p=0.32) & \textbf{0.0} (0.0/0.0/p=0.0) & \textbf{28.2} (0.7/0.1/p=0.03) \\ 
l3r3 \footnotesize{(mono/replace)} & \textbf{68.9} (0.6/0.1/p=0.8) & \textbf{0.0} (0.0/0.0/p=0.0) & \textbf{28.2} (0.7/0.0/p=0.01) \\ 
\hline
Baseline  \footnotesize{(append)}& 68.8 (0.6/0.1/p=-) & \textbf{0.0} (0.0/0.0/p=-) & 28.3 (0.7/0.0/p=-) \\ 
l1r1 \footnotesize{(mono/append)} & \textbf{68.9} (0.6/0.1/p=0.12) & \textbf{0.0} (0.0/0.0/p=0.0) & 28.3 (0.7/0.1/p=0.56) \\ 
l2r1 \footnotesize{(mono/append)} & 68.8 (0.6/0.1/p=0.9) & \textbf{0.0} (0.0/0.0/p=0.0) & 28.3 (0.7/0.1/p=0.32) \\ 
l2r2 \footnotesize{(mono/append)} & 68.8 (0.6/0.1/p=0.76) & \textbf{0.0} (0.0/0.0/p=0.0) & \textbf{28.2} (0.7/0.1/p=0.02) \\ 
l3r3 \footnotesize{(mono/append)} & 68.8 (0.6/0.0/p=0.69) & \textbf{0.0} (0.0/0.0/p=0.0) & \textbf{28.2} (0.7/0.0/p=0.04) \\ 
\hline
\end{tabular}}
\caption{Results on EMEA, English to Spanish and English to Dutch}
\label{tab:emea}
\end{table}


\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER} \\ 
\hline
\multicolumn{4}{|c|}{JRC-en-es} \\
\hline
Baseline  \footnotesize{(replace)}& 57.6 (0.5/0.2/p=-) & \textbf{73.1} (0.4/0.0/p=-) & 32.7 (0.5/0.2/p=-) \\ 
l1r1 \footnotesize{\footnotesize{(mono/replace)}} & 57.2 (0.5/0.1/p=0.0) & 72.2 (0.4/0.1/p=0.0) & 33.1 (0.5/0.4/p=0.0) \\ 
l2r1 \footnotesize{(mono/replace)} & 57.6 (0.5/0.1/p=0.74) & 72.5 (0.4/0.1/p=0.0) & 32.7 (0.5/0.2/p=0.86) \\ 
l2r2 \footnotesize{(mono/replace)} & 57.7 (0.5/0.1/p=0.0) & 72.6 (0.4/0.1/p=0.0) & 32.8 (0.5/0.2/p=0.23) \\ 
l3r3 \footnotesize{(mono/replace)} & \textbf{57.8} (0.5/0.0/p=0.01) & 72.7 (0.4/0.0/p=0.0) & \textbf{32.5} (0.5/0.2/p=0.0) \\ 
l1r1 \footnotesize{(experts/replace)} & 57.2 (0.5/0.1/p=0.0) & 72.1 (0.4/0.0/p=0.0) & 33.4 (0.5/0.2/p=0.0) \\ 
l2r1 \footnotesize{(experts/replace)} & 57.0 (0.5/0.4/p=0.0) & 72.2 (0.4/0.1/p=0.0) & 33.5 (0.5/0.4/p=0.0) \\ 
l2r2 \footnotesize{(experts/replace)} & 57.6 (0.5/0.2/p=0.26) & 72.5 (0.4/0.1/p=0.0) & 32.8 (0.5/0.4/p=0.03) \\ 
l3r3 \footnotesize{(experts/replace)} & \textbf{57.8} (0.5/0.0/p=0.0) & 72.7 (0.4/0.1/p=0.0) & 32.7 (0.5/0.1/p=0.89) \\ 
\hline
Baseline  \footnotesize{(append)}& 57.6 (0.5/0.1/p=-) & \textbf{73.1} (0.4/0.0/p=-) & 32.8 (0.5/0.1/p=-) \\ 
l1r1 \footnotesize{(mono/append)} & 57.6 (0.5/0.1/p=0.07) & 72.7 (0.4/0.0/p=0.0) & 32.7 (0.5/0.1/p=0.18) \\ 
l2r1 \footnotesize{(mono/append)} & 57.6 (0.5/0.1/p=0.02) & 72.8 (0.4/0.1/p=0.0) & 32.7 (0.5/0.2/p=0.05) \\ 
l2r2 \footnotesize{(mono/append)} & 57.5 (0.5/0.1/p=0.25) & 72.7 (0.4/0.1/p=0.0) & 32.9 (0.5/0.0/p=0.04) \\ 
l3r3 \footnotesize{(mono/append)} & 57.5 (0.5/0.3/p=0.58) & 72.7 (0.4/0.1/p=0.0) & 32.9 (0.5/0.3/p=0.02) \\ 
l1r1 \footnotesize{(experts/append)} & \textbf{57.7} (0.5/0.1/p=0.07) & 72.8 (0.4/0.1/p=0.0) & 32.7 (0.5/0.1/p=0.29) \\ 
l2r1 \footnotesize{(experts/append)} & \textbf{57.7} (0.5/0.1/p=0.0) & 72.8 (0.4/0.0/p=0.0) & \textbf{32.6} (0.5/0.2/p=0.0) \\ 
l2r2 \footnotesize{(experts/append)} & 57.5 (0.5/0.3/p=0.44) & 72.7 (0.4/0.1/p=0.0) & 32.9 (0.5/0.2/p=0.01) \\ 
l3r3 \footnotesize{(experts/append)} & 57.6 (0.5/0.3/p=0.0) & 72.8 (0.4/0.0/p=0.0) & 32.7 (0.5/0.4/p=0.05) \\ 
\hline
\end{tabular}}
\caption{Results on JRC, English to Spanish}
\label{tab:jrc}
\end{table}


From all sets of experiments, we again observe that the results do not surprass
the baseline, although for JRC some configurations such as \emph{l2r1
\footnotesize{(experts/append)}} do score slightly above threshold, with statistical
significance, on two out of three metrics. It is clear by now that surpassing the
non-context informed baseline seems a very tough problem.

We have tried additional experiments on different corpora and language pairs. A
Frisian-Dutch parallel corpus was assembled on mostly literary texts. This
language pair benefits from high BLEU scores due to the high similarity between
the two languages. Running our experiments on this corpus, both for Dutch to
Frisian as well as Frisian to Dutch scores below baseline, and the TER score
that does exceed baseline does not pass the significance tests.
Table~\ref{tab:fa} shows an excerpt of these experimental results.

\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER} \\ 
\hline
\multicolumn{4}{|c|}{FryskeAkademy-nl-fy} \\
\hline
Baseline  \footnotesize{(replace)}& \textbf{60.7} (0.6/0.1/p=-) & \textbf{73.9} (0.4/0.0/p=-) & 33.6 (0.6/0.1/p=-) \\ 
l1r1 \footnotesize{(mono/replace)} & 60.3 (0.6/0.1/p=0.0) & 73.7 (0.5/0.0/p=0.0) & \textbf{33.5} (0.6/0.1/p=0.2) \\ 
l2r1 \footnotesize{(mono/replace)} & 60.3 (0.6/0.1/p=0.0) & 73.7 (0.5/0.0/p=0.0) & \textbf{33.5} (0.6/0.1/p=0.2) \\ 
\hline
\multicolumn{4}{|c|}{FryskeAkademy-fy-nl} \\
\hline
Baseline  \footnotesize{(replace)}& \textbf{55.3} (0.7/0.0/p=-) & - & \textbf{28.6} (0.6/0.0/p=-) \\ 
l1r1 \footnotesize{(mono/replace)} & 54.9 (0.6/0.0/p=0.0) & -& 28.8 (0.5/0.0/p=0.0) \\ 
l2r1 \footnotesize{(mono/replace)} & 54.9 (0.6/0.1/p=0.0) & -& 28.9 (0.6/0.0/p=0.0) \\ 
\hline
\end{tabular}}
\caption{An excerpt of the results on the Frisian parallel corpus, Dutch to Frisian and Frisian to Dutch. For the latter, no METEOR scores are available} %TODO: why?
\label{tab:fa}
\end{table}

A possible hypothesis we entertained is that source-side context modelling
stands to gain most when translating from morphologically simpler languages to
morphologically richer languages, as the source-side context information may
help in determining the proper morphological form. We test this with English to
Russian on the Yandex corpus, but consistently score below baseline. An excerpt
of these results is shown in Table~\ref{tab:yandex}.


\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lccc}
\hline
\textbf{System} & \textsc{BLEU}  & \textsc{METEOR}  & \textsc{TER} \\ 
\hline
\multicolumn{4}{|c|}{Yandex English-Russian} \\
\hline
Baseline  \footnotesize{(replace)}& \textbf{15.7} (0.3/0.0/p=-) & -& \textbf{72.0} (0.3/0.0/p=-) \\ 
l1r1 \footnotesize{(mono/replace)} & 14.7 (0.3/0.0/p=0.0) & -& 73.3 (0.3/0.0/p=0.0) \\ 
l2r1 \footnotesize{(mono/replace)} & 15.0 (0.3/0.0/p=0.0) & -& 73.0 (0.3/0.1/p=0.0) \\ 
\hline
\end{tabular}}
\caption{An excerpt of the results on the Yandex corpus, English to Russian. No METEOR scores are available.} %TODO: why?
\label{tab:yandex}
\end{table}


\subsection{Quantitative Analysis}
\label{sec:quantanal}

Are we placing our bets on the wrong horse by focussing solely on $p(t|s)$ and
\emph{replacing} it with $p(t|s,C)$? No, if we aggregate the results from all
experiments that use the replace score weighting method, we find that the
$p(t|s)$ or $p(t|s,C)$ feature receives the highest weight of the translation
model scores in $63\%$ of the experiments\footnote{Multiple
optimiser runs (3) are counted as separate experiments in this test}, the next runner-up with $25\%$ is the forward probability
$p(s|t)$. 

We run the same test for the append score weighting method, which leaves the $p(t|s)$ feature as-is, and
\emph{appends} the $p(t|s,C)$ feature to the translation model.  We find that
$p(t|s,C)$ still receives most weight in $33\%$ percent of the experiments.
The unaltered $p(t|s)$ score wins in $29\%$ of the cases and $p(s|t)$ in
$23\%$ \footnote{The remaining probability mass is distributed over the lexical
weighting features}.

These tests show that our focus on this particular feature in the
translation model is justified.

%In another test, we observe whether the language model receives more weight than the highest
%ranking feature in the translation model, 

We have measured the impact of the classifier on translation quality.  To get a
deeper understanding of the impact of the classifier(s), we measure to what
extent the phrases used in the test set output of a context-informed system
differ from the phrases used in the non-context informed output, irrespective
of whether that difference contributes positively or negatively to the
translation quality. To this end, we ask Moses to verbosely output, for each
output sentence, which phrases from the intermediate phrase-translation table
are used. We then analyse how many phrase pairs are unique to the
context-informed system.

These phrases are indicative of the SMT decoder
choosing a different translation option than it did for the baseline, due to
either the classifier information or optimiser differences.
Table~\ref{tab:decodediff} shows the results for a few of the data sets we
used. We also show, in the last column, how many of the source-side fragments
are not in the baseline, implying that the SMT decoder chose a different
source-phrase to translate than the baseline did.

For this experiment, however, we had to take the optimiser out of the equation
by optimising only once on the non-context-informed baseline, and use thise
parameters on the context-informed experiments as well. Although this leads to
sub-optimal translation quality, it guarantees that the difference we measure
can be attributed solely to the inclusion of context information:


\begin{table} 
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{lllcc}
\hline
\textbf{Corpus} & \textbf{Language Pair} & \textbf{Configuration} & \textbf{Diff. pairs} & \textbf{Diff. source} \\
\hline
%old data, optimized on baseline only (on purpose!):
Yandex (1M) & English$\rightarrow$Russian & l1r1 \footnotesize{(mono/replace)} & $37.72\%$ & $26.58\%$ \\
Fryske Akademy & Frisian$\rightarrow$Dutch & l1r1  \footnotesize{(mono/replace)}& $33.57\%$ & $32.50\%$ \\
IWSLT 2012 TED & Dutch$\rightarrow$English & l1r1  \footnotesize{(mono/replace)}& $26.86\%$ & $20.07\%$ \\
IWSLT 2006 & Chinese$\rightarrow$English & l1r1  \footnotesize{(mono/replace)}& $24.55\%$ & $12.49\%$ \\
%new data, but changes may be due to optimiser differences:
%EMEA & English$\rightarrow$Spanish & l1r1 (monolithic/replace) & $15.73\%$ & $14.65\%$ \\
%JRC & English$\rightarrow$Spanish & l1r1 (monolithic/replace) & $41.59\%$ & $35.90\%$ \\
%Yandex (1M) & English$\rightarrow$Russian & l1r1 (monolithic/replace) & $48.92\%$ & $37.30\%$ \\
%Fryske Akademy & Frisian$\rightarrow$Dutch & l1r1 (monolithic/replace) & $41.51\%$ & $40.07\%$ \\
%IWSLT 2012 TED & Dutch$\rightarrow$English & l1r1 (monolithic/replace) & $43.78\%$ & $30.29\%$ \\
%IWSLT 2006 & Chinese$\rightarrow$English & l1r1 (monolithic/replace) & $43.78\%$ & $30.29\%$ \\
\hline
\end{tabular}}
\caption{The number of phrases in the context-informed test output that are not
in the non-context-informed baseline.} 
\label{tab:decodediff}
\end{table}

We observe from this data that our system does have a decent impact on phrase
selection. A correlation between translation quality and the difference ratio
of phrase pairs or source phrases, can not be discerned, however.

\subsection{Classifier type}
\label{sec:typeopt}

We have introduced two types of classifiers, integrated in a Statistical
Machine Translation framework by means of the ``bypass'' method described in
Section~\ref{sec:smtintegration}. The monolithic classifier, modelling all phrase-pairs
in a single classifier, is used also by \cite{Stroppa+07} and
\cite{Rejwanul+11}, whereas the classifiers experts, one classifier per
source-phrase, is a new addition that more closely resembles how such
classifiers are employed in Word Sense Disambiguation.

Classifier experts were built using the IB1 algorithm, whereas we choose for
TRIBL2 for the monolithic classifier. This was done because the monolithic
classifier is a lot bigger by definition; a pure IB1 approach would be too slow
and TRIBL2 seeks to combine the accuracy of IB1 with the speed of a
decision-tree classifier, IGTree.

Table~\ref{tab:expertcount} lists the number of classifier experts that were
built for some of the data sets. The actual number of source phrases is always
much higher, as classifiers are only built when there is ambiguity regarding
the translation. To see how these classifiers are typically distributed, based
on the number of instances and classes, we zoom in on the IWSLT 2012 data for
Dutch to English, and plot a histogram in
Figure~\ref{fig:histogram}. A typical Zipfian curve can be discerned from
this figure, with the vast majority of classifier experts being based
on a small amount of instances (left) and a long tail of experts, only
partially shown, with fewer than 100 instances. Only a small subset of
these experts has more than one translation option, seen by the small
amount of red in the right half of Figure~\ref{fig:histogram}. In
other words, most ambiguity is present in classifier experts trained on less than
100 examples of a source phrase in different contexts. 
%having few translations options, i.e classes, and an immediate sharp drop in
%frequency as the number of instances or classes increases.

\begin{table}
\noindent\makebox[\textwidth][c]{%
\begin{tabular}{llll}
\hline
\textbf{Corpus} & \textbf{Sentence pairs} & \textbf{Languages} & \textbf{Classifier Experts} \\
\hline
%Europarl & 250,000 & English-Spanish & 828,326 \\
JRC-Acquis & 1,233,670 & English-Spanish & 12,652,153 \\
%EMEA & 1,088,333 & English-Spanish & n/a \\ %not run with experts?
%EMEA & 1,080,084 & English-Dutch & n/a  \\ %not run with experts?
IWSLT 2012 TED & 127,806 & Dutch-English & 193,811 \\
IWSLT 2006 & 40,274 & Chinese-English & 22,212 \\
Fryske Akademy & 137,937 & Dutch-Frisian & 276,819 \\
Fryske Akademy & 137,937 & Frisian-Dutch & 213,573 \\
\hline
\end{tabular}}
\caption{Number of classifier experts generated per data set. 
%Classifier expert count is not available for EMEA, as it was only run using the monolithic classifier.
}
\label{tab:expertcount}
\end{table}


\begin{figure}
\begin{center}
\newlength\figureheight
\newlength\figurewidth
\setlength\figureheight{6cm}
\setlength\figurewidth{8cm}
\input{histogram.tikz}
\caption{Histogram showing the distribution of the number of instances (blue)
and number of classes (red) in the classifier experts for IWSLT 2012 TED,
Dutch to English (logarithmic).}
\label{fig:histogram}
\end{center}
\end{figure}

Various of the tables in Section~\ref{sec:results1} list results for both classifier
types. As both methods generally fail to surpass the baseline, and the
differences in translation score between classifier experts and monolithic
classifier appear minimal, we can not reach a strong conclusion regarding their
respective merit.

%The case for the classifier experts can be motivated by the fact that feature weights are computed on a
%per-source-phrase basis, rather than on the aggregation of all. It can be
%considered as a kind of optimisation step, in absence of classifier parameter
%optimisation and explicit variable context selection (see Section~\ref{sec:paropt}). The
%disadvantage is that such systems may be more prone to overfitting, which might
%well be the case in the EMEA experiment.


\subsection{Weighting methods and score analysis}
\label{sec:weighting}

Two weighting methods have been implemented: the \emph{append} method, adding
$p(t|s,C)$ as an extra score to the score vector, and the \emph{replace}
method, which replaces the existing $p(t|s)$ score with $p(t|s,C)$.

Comparing the two weighting methods, however, is not trivial.  The very act of
adding a score, for the append method, shifts the weights for the score vector.
Any shift may likely affect the outcome. This is also the reason that the
replace and append methods have their own respective baselines in the result
tables seen so far.

For the append method, the $p(t|s,C)$ score is often simply
equal to the $p(t|s)$ score, this is by definition so for any source phrases for which no
classifier was needed, due to not being ambiguous in translation or context. 

As seen before, given the over-all lack of impact above baseline observed across most
experiments, we can not draw any satisfactory conclusion on the merit of the
\emph{append} method versus the \emph{replace} method, given the experiments we
conducted.

We can, however, use the append scoring method to provide further insight into
the classifier results. How often does the classifier assign a $p(t|s,C)$ that
differs from $p(t|s)$? And how big is this difference?

For each occurrence of each source phrase in a given test set, we gather all
possible translations from the phrase-translation table and compute the
difference $\Delta$ between the context-informed translation probability and
the non-context informed one as per Equation~\ref{eqn:scorediff}.

\begin{equation}
  \Delta = 
\begin{cases}
 \frac{p(t|s,C)}{p(t|s)} - 1, & \text{if } p(t|s,C) \geq p(t|s)  \\
 -(\frac{p(t|s)}{p(t|s,C)} - 1), & \text{otherwise}
\end{cases}
\label{eqn:scorediff}
\end{equation}


We compute this on the test data of the JRC-Acquis corpus,
from English to Spanish, as shown in Figure~\ref{fig:scoredifference}. For
training we used a reduced subset of $250,000$ sentences. The measurements are taken
from the phrase-translation table directly so the decoder and optimizer plays
no role here. Our measurement merely assesses to what extent context-informed classifiers

For Figure~\ref{fig:scoredifference}, we set up bins of size 0.1, so each point
on the $x$ axis aggregates results of the specified $\Delta \pm 0.05$. A
value of zero indicates there is no difference whatsoever, this includes all
source phrases where either translation or context are unambiguous. A spike and
outlier can be observed at this point, covering $9\%$ of
all phrase-pairs. We observe that the positive side of the graph drops much
less sharply than the negative side. The positive numbers cover the phrase
pairs to which the classifier assigns a higher probability; the value expresses
the factor by which the probability is increased. No less than $75\%$ of the
phrase pairs are in this region. This is a direct result of the discriminative
filtering done by the classifiers, which prunes the weakest translation
options, and therefore on the whole raises the probability of the remaining
options.

\begin{figure}
\begin{center}
\setlength\figureheight{5cm}
\setlength\figurewidth{5cm}
\input{scoredifference.tikz} \input{scoredifference2.tikz}
%\includegraphics[width=90.00mm]{scoredifference.png}
\caption{Scatter plot (logarithmic scale) illustrating the difference $\Delta$
between the classifier score and the original SMT score on the JRC-Acquis
 test set, English to Spanish, trained on 250,000 sentences. The second figure provides a zoomed view and illustrates that small changes are most prevalent.}
\label{fig:scoredifference}
\end{center}
\end{figure}


\subsection{Qualitative analysis}
\label{sec:qualanal}

In this section we take a closer look at the translations themselves and see if
we can discern patterns that can tell us what the effect of source-side context
information is and when it pays off.

We zoom in on the EMEA corpus -- English to Spanish, one million sentence
pairs. We compare a special run of the context-informed \texttt{l1r1} system
with the non-context informed baseline in order to assess the impact of context
information. In this special run, the decoder parameters have not been
optimised independently, but are set equal to those of the baseline. Although
this leads to sub-optimal translation quality, it ensures we compare only the
effect of the classifier rather than of the optimiser. Out of $5,000$ sentences
in the test set, $3,863$ ($77\%$) are translated identically. This high number
again shows that it is difficult to make a difference. Nevertheless, $23\%$
differs.

The next few examples will show some comparisons in which the context-informed
system improves over the baseline. 

Example~\ref{ex:QAsynonym} shows three examples of the selection of a better
translation given the context, matching with the reference translation, even
though the baseline translation could be considered correct as well. This is
the most common type of difference. Example~\ref{ex:QAdrop} shows the dropping
of a word. Example~\ref{ex:QAgrammar} shows selection of a grammatically better
phrase in the context.


\begin{exe}
\footnotesize
\ex \textbf{Baseline:} El cambio continuo del lugar de inyección dentro de \underline{un área} determinada puede ayudar a \\
\textbf{l1r1:} El cambio continuo del lugar de inyección dentro de \underline{una región} determinada puede ayudar a \\
\textbf{Reference:} La contínua rotación de los puntos de inyección dentro de \underline{una región} determinada puede ayudar a reducir o prevenir estas reacciones \\
\noindent\makebox[\linewidth]{\rule{\linewidth}{0.4pt}}
\textbf{Baseline:} Baraclude reduce la cantidad de virus en su \underline{cuerpo} y mejora el estado del hígado . \\
\textbf{l1r1:} Baraclude reduce la cantidad de virus en su \underline{organismo} y mejora el estado del hígado  \\
\textbf{Reference:} Baraclude reduce la cantidad de virus en su \underline{organismo} y mejora el estado del hígado .
\\ 
\noindent\makebox[\linewidth]{\rule{\linewidth}{0.4pt}}
\textbf{Baseline:} Como consecuencia , el fenilbutirato de sodio , reduce \\  \underline{los niveles plasmáticos} de amonio y glutamina en pacientes con trastornos del ciclo de la urea . \\
\textbf{l1r1:} Como consecuencia , el fenilbutirato de sodio reduce \\ \underline{las concentraciones plasmáticas elevadas} de amonio y glutamina en pacientes con trastornos del ciclo de la urea .  \\
\textbf{Reference:} Como consecuencia , el fenilbutirato de sodio reduce \\ \underline{las concentraciones plasmáticas elevadas} de amoníaco y glutamina en pacientes con trastornos del ciclo de la urea .
\label{ex:QAsynonym}
\end{exe}

\begin{exe}
\footnotesize
\ex \textbf{Baseline:} \underline{Los estudios} en animales , no pueden excluir el desarrollo potencial de toxicidad ( ver sección 5.3 ) . \\
\textbf{l1r1:} \underline{Estudios} en animales , no pueden excluir el desarrollo potencial de toxicidad ( ver sección 5.3 ) . \\
\textbf{Reference:} \underline{Estudios} en animales , no pueden excluir el desarrollo potencial de toxicidad ( ver sección 5.3 ) .
\label{ex:QAdrop}
\end{exe}

\begin{exe}
\footnotesize
\ex \textbf{Baseline:} Las reacciones adversas \underline{consideradas relacionadas} con el uso de Agenerase son síntomas gastrointestinales , erupción y parestesia oral / peri-oral . \\
\textbf{l1r1:}  Las reacciones adversas \underline{que se consideran relacionadas} con el uso de Agenerase son síntomas gastrointestinales , erupción y parestesia oral / \\
\textbf{Reference:} Las reacciones adversas \underline{que se consideran relacionadas} con el uso de Agenerase son síntomas gastrointestinales , erupción y parestesia oral / perioral . \\ \\
\label{ex:QAgrammar}
\end{exe}

In addition to the positive examples, there are also neutral and negative
examples. In example~\ref{ex:QAneutral}, all translations can technically be
considered correct and convey the same message with different nuances; a
different construction is chosen. Example~\ref{ex:QAnegative} shows the
inverse situation of example~\ref{ex:QAsynonym}; a different synonym was chosen
but it does not match with the reference translation. This too is a common
pattern in the data. This raises the question whether the impact of
context-information would not be lower if the test set would have contained
multiple reference translations. Such data unfortunately is hard to come by and
was not available in this study.

\begin{exe}
\footnotesize
\ex \textbf{Baseline:} Asegúrese de que el polvo \underline{esté completamente disuelto} . \\
\textbf{l1r1:} Asegúrese de que el polvo \underline{se disuelva completamente} .  \\
\textbf{Reference:} Asegúrese de que el polvo \underline{se ha disuelto completamente} .
\label{ex:QAneutral}
\end{exe}

\begin{exe}
\footnotesize
\ex \textbf{Baseline:} Los pacientes \underline{se asignaron aleatoriamente} a recibir 500 $\mu$ g de Aranesp una vez cada tres semanas o 2,25 $\mu$ g / kg una vez a la semana . \\
\textbf{l1r1:} Los pacientes \underline{fueron aleatorizados} a recibir 500 $\mu$ g de Aranesp una vez cada tres semanas o 2,25 $\mu$ g / kg una vez a la semana . \\
\textbf{Reference:} Los pacientes \underline{se asignaron aleatoriamente} a recibir 500 $\mu$ g de Aranesp una vez cada tres semanas o 2,25 $\mu$ g / kg una vez a la semana .
\label{ex:QAnegative}
\end{exe}


\section{Conclusions and Discussion} 
\label{sec:conclusion}

We have conducted a large number of experiments to assess whether surface-form
source-side context information, i.e. without the use of any explicit
linguistic features that require supervised parsers or taggers, can improve
translation quality. Memory-based classifiers were used, and integrated in an
SMT framework, following techniques commonly employed in WSD. Different
classifier types, context sizes, and score weighting methods have been
researched. Multiple parameter optimisation runs were conducted to obtain the
decoder parameter weights, compensating for decoder instability. Optimisation
of classifier parameters and feature selection was omitted in order to contain
the computational complexity of the problem, and considering the lack of positive
results for optimisation of classifier parameters in Chapter~\ref{chap:clwsd}.

Various distinct corpora and language pairs have been employed for the
experiments in order to ensure that conclusions are of a generic enough
nature rather than incidental artifacts of the type that have arguably steered
conclusions in previous work. Despite all efforts, clear positive results were only
attained for IWSLT 2006 (Chinese to English). Other scores did not manage to
unequivocally surpass the baseline.

%We hypothesised that source-side context information may contribute more when
%translating to a morphologically more complex language, but this was
%empirically refuted.  An alternative hypothesis was that a certain quality of
%translation had to be reached before the source-side context can make an
%impact. This hypothesis could not be confirmed on Dutch-Frisian data, and is at
%odds with the positive results on IWSLT 2006 English-Chinese, which had
%relatively little data and low scores.

In this study, we effectively replicated and expanded upon the part of the work
of \cite{Stroppa+07} and \cite{Rejwanul+11} that does not depend on further
linguistic information. Despite a small number of positive results reported in these
studies, we have to conclude that inclusion of surface-form source-side context
information is not a good route for the improvement of MT quality. It is likely
that improvement can be more easily achieved through for example optimisation
of the target-side language model.

Finally, we would like to focus on why the results of our study
overall do not exceed the baseline.  There may be an explanation for this
if we look at the interplay between the SMT decoder and the classifiers. The
classifier instances are generated with the phrase-translation table as input,
looking up the context in the corpus. There will therefore never be phrases in
the classifier training data that do not occur in the classifier. If we have a
classifier training instance $(s,C) \mapsto t$, i.e.  a source phrase $s$ with
context $C$ and mapping to translation $t$, and $C$ is a context that occurs
multiple times in the corpus, then there is often a source fragment $s'$ in the
phrase-translation table that is the conjunction of $s$ and $C$, and which maps
to a $t'$ of which $t$ is at least a substring. In other words, if a context
for a source phrase is sufficnetly attested in the data, then this context may be
an integral part of a larger source phrase. Only if the context is not
prevalent at all, then no such $s'$ exists, in which case one may also posit
that the context is less likely to occur in test data and the training instance
is therefore considered fairly weak.  It is thus likely that the context
information we try to \emph{explicitly} model, is often already
\emph{implicitly} available to the decoder.  Rather than considering a source
phrase in context, the decoder may thus opt to choose to include the context as
integral part of the phrase.  The target-language model already plays an
important role here, as it encourages the decoder to choose translation options
that fit well together. Source-side context modelling, without additional
linguistic features, seems to have little to nothing to offer.

