\chapter{Introduction}
\label{chap:intro}

“To know an object is to lead to it through a context which the world provides.” -- William James (American Philospher, 1861-1947)

"Form is emptiness, emptiness is form" -- Heart Sutra

\section{Context as Linguistic Bridges}


To even begin to merit the title of ``Doctor of Philosophy'', it is only proper
to start this dissertation with some philosophical deliberations.

The study you are about to read sprung from the intuition that \emph{context}
is an important, if not the most important, characteristic that defines
language. Language itself only exists only in the context of the world that
surrounds us, as well our internal mental world. Without this context, there is
nothing to communicate in the first place. Language is sometimes considered the
epitome of human evolution. Our species evolved the remarkable ability to refer
to the world outside and within by producing complex meaningful utterances,
i.e.\ speech.


This was an unparalleled revolution in communication, which has
undoubtedly played a leading role in us becoming the dominant intelligent
species on the planet. It has put us in a position where we can communicate our
thoughts and feelings about the world to one another on a more fine-grained
level than any other species can. It has given us the ability of
ever-increasing abstraction. The context of our language is no longer
limited to just refer to objects in our immediate vicinity, but we can even
refer to abstract thought itself. Whenever communication is attempted between
people who do not share a language, the context to which can be referred is
restricted.  Just imagine tourists in a foreign bakery lustfully
pointing at the pastries they desire to buy and devour.

Another major revolution in communication has been the development of writing,
and much later that of print technology. This and the accompanied literacy of
populations allows for our thoughts and ideas to be preserved more easily and
accurately than oral traditions can accomplish. The ability to read and write
broadens one's world, one's context, and receiving language education is
therefore even deemed a fundamental human right.

Language is inherently ambiguous and context is the disambiguating factor
without which it cannot exist. The context of a bakery and a neatly arranged
line of pastries is essential for the baker to be able to disambiguate the
pointing of the clueless foreign tourist and discern which pastry he actually
wants. Demonstrative pronouns such as ``he'', ``she'', ``this'', or even
definite noun phrases such as ``the house'' convey little information, if not
for the context they are employed in.  In fact, any word by itself is pretty
limited, can never exist in total isolation, and only derives meaning from the
further context. It is defined through the context, just like a
dictionary defines words in terms of other words.

%If the context of your own
%upbringing was Indian and you had been brought up learning how to read
%devanagari script, you would probably be able to read that as ``shunyata''.
%Merely being able to read and vocalize the word still wouldn't bring you closer
%to its meaning though.
%The word is Sanskrit, though it comes from a Pali word,
%as is the form in which it is written in an Buddhist scripture called the
%``Heart Sutra''. The fact we can can even still talk about this word and its usage in a
%2500-year old text today is already a testimony to the major impact writing
%technology has on our civilization. Hundreds of millions of adherents of
%various major religions still derive their daily religious practice from
%throughts and ideas put down in writing in ancient times. Moreover, as a
%scientific community, we base our studies on the work of our predecessors,
%explicitly citing them as our methodology requires. But I digress, as my
%main point relates to the meaning of this word.

Buddhist philosophy has a concept called \emph{``sh\=unyat\=a''}. Sh\=unyat\=a is Sanskrit and can
roughly be translated into English as ``emptiness''. It is an ontological
notion that states that that everything, not just words, is interrelated and nothing has any
existence or essence of or by itself: this philosophy posits that everything
exists and is defined purely through \emph{its context} and can by definition
never be seen as independent from it.

\subsection*{Linguistic bridges}
\label{sec:bridges}

Whereas we humans all share an astounding capacity for language, history has
given rise to numerous distinct and often mutually unintelligible languages.
This brings us back to the predicament of our pastry-loving tourist in the
foreign bakery, unable to express his choice in the language of the baker and
therefore resorting to pointing. If we situate the bakery in France, the baker
may reply ``croissant'' in response to the gesturing of his client. The
perceptive client may then have learned that this is what the pastry is called.
The famous utterance ``Me Tarzan, You Jane'', heavily supported by gestures as
in the movies, is of a similar nature. Both protagonists breach the language
barrier and gain new information. These examples show that context plays an
important role in establishing translation, or any common vocabulary.

The \emph{linguistic bridges} of this dissertation refer to these acts of
translation. We can define translation as \emph{a process that yields a
representation of a message initially expressed in one language, in another}.
Etymologically, the word \emph{translation}, from the latin \emph{transl\=ati\=o},
refers to carrying (l\=ati\=o) across (trans) something. The focus is generally on
accurate preservation of the semantics of message, although in some arts, such
as poetry, form or emotion may take precendence over the substance.

The intuition underlying our the present study is that the context of a word or phrase is
an important cue for the translation of that word or phrase. A word in context
A may be translated differently than the same word in context B. Consider the
Portuguese word ``tempo'' in the following sentences:

\begin{enumerate}
\item ``O \textbf{tempo} é bom hoje.'' -- ``The \textbf{weather} is nice today.''
\item ``Não tenho \textbf{tempo} para estudar hoje.'' -- ``I don't have \textbf{time} to study today.''
\end{enumerate}

In the first sentence, ``tempo'' is translated as ``weather'', whereas in the
second it is translated as ``time''. Portuguese uses the same word where
English uses two distinct words; context provides cues to disambiguate into the
proper translation. Another example shows where the relationship between English words and their French counterpart is
not one-on-one and requires context to properly disambiguate:

\begin{enumerate}
\item ``J'ai tué la \textbf{mouche}'' -- ``I killed the \textbf{fly}''
\item ``Je \textbf{vole} à Paris'' - ``I \textbf{fly} to Paris''
\item ``Je \textbf{vole} l'horloge de mon père'' -- ``I \textbf{steal} my father's watch''
\end{enumerate}

In this latter example, disambiguation between the first two sentences is
facilitated by the fact that the word ``fly'' has a different
part-of-speech in both sentences, unlike the noun ``tempo'' in the earlier example.
The presence of the article ``the'' in the noun phrase ``the fly'' already
rules out translation to a verb. The French verb ``voler'' in turn does not
just translate to ``to fly'', but can also mean ``to steal'', as shown in
the last sentence.

These examples illustrate that languages are almost never translatable on a
naive word-by-word basis, leaving aside the issue of word order, as
determined by a language's syntax. Context plays a major role in determining
the right translation. We can say context fulfills a bridging function in
translation, and this dissertation investigates techniques for machines to
exploit this to attain better automatic translations.

\section{Natural Language Processing: Background}

In our philosophical deliberations we briefly addressed the communication
revolutions brought about by humanity evolving the faculty of language and
speech, and later the technological development of writing and print. We all
live in fortunate times to have just witnessed the next biggest revolution in
communication since the invention of print: the information revolution. Text is
not just printed anymore, it is digitised; i.e.\ made available, distributable,
and searchable in digital form. The internet enables these digital resources to
be collected, shared, and exploited at unpredecented rate and allows us to
communicate instantaneously with people from all over the planet.

%To emphasize the importance of this development, and to underline my personal
%conviction that this is the way the future should go; this dissertation itself
%will be made available primarily in an open digital form, rather than printed
%book form.

This wealth of digital textual data fuels our field of research: Natural Language Processing (NLP). We attempt to
extract meaningful patterns from natural language data.  Most contemporary techniques in NLP are grounded in Machine
Learning, which employs statistical and information-theoretical principles to learn desired patterns from big
collections of natural language data. These data-driven approaches in NLP can be contrasted to rule-based approaches,
not without their own merit, that are directly derived from human expert knowledge.

This dissertation strictly follows the data-driven trend and we therefore rely
on large amounts of digitised text to conduct our research. In this section we
give some theoretical background on \emph{Machine Translation} and \emph{Word
Sense Disambiguation}, in order to be able to understand the subject matter of
this dissertation.

\subsection*{Machine Translation}
\label{sec:intromt}

The area of research dedicated to the automatic translation of data from one natural language to another is called
Machine Translation (MT).  At the time the study of this dissertation is conducted, state-of-the-art techniques in this
field are primarily based on statistical methods, in which case we can speak of \emph{Statistical Machine Translation}
(SMT). These can contrasted to earlier rule-based Machine Translation systems, which are characterised by an
\emph{analysis} stage, a \emph{transfer} stage according to a set of rules, and a \emph{generation} stage. Such transfer
may occur on a shallow, a syntactic or a semantic level.

Statistical Machine Translation, in contrast, treats translation as a problem where an
ordered sequence of words or phrases has to be found that is the most
\emph{probable} translation of the input sentence. It effectively tests many possible
translations for individual fragments, and various ways in which these
translated fragments can be ordered. Each test is a \emph{translation
hypothesis}, and many such hypotheses are evaluated by the system
for a single sentence. The one with the highest probability score is eventually
selected.

Statistical Machine Translation is an integrated solution that handles two
problems at once; that of finding the best lexical translation for words or
phrases, and that of reassembling those in an acceptable order. This corresponds to
two key properties of translation:

\begin{enumerate}
	\item \textbf{Semantic faithfulness} - A translation must accurately transfer meaning for all of its parts. The right lexical translation has to be found for the words and expressions in the source.
	\item \textbf{Syntactic fluency} - The translated result must be fluently
        expressed, the sentence must be syntactically well-formed.
\end{enumerate}

In Statistical Machine Translation, two key models tackle these issues. The
\emph{translation model} models the translation of words or phrases from the
source language to the target language. The \emph{language model}, for the
target language, provides the means to order reassembled translated fragments. Both
are probablistic models that are learned from data. The translation model
requires a \emph{parallel corpus} as input, and the language model can be
trained on monolingual data.

As said, the act of translation is expressed as the search for the most
probable translation. Take $S$ to be a sentence in the source language, and $T$
to be a translated sentence, i.e.\ a translation hypothesis, in the target
language. The SMT process can then be formalised as in
Equation~\ref{eq:introsmt1}, where the outcome $\hat{T}$ corresponds to the most probable translation.

\begin{equation}
\hat{T} = \argmax_T p(T|S)
\label{eq:introsmt1}
\end{equation}

By applying Bayes' theorem we can invert this to Equation~\ref{eq:introsmt2}.
This inversion is necessary as we cannot model $p(T|S)$ directly. Instead, we
apply what is known as the \emph{noisy channel model}. The system generates a
large amount of translation hypotheses. This can be interpreted as if our
original sentence in the source language entered a noisy channel and comes out
scrambled in the target language at the other end. We then compute how probable
a translation it is of the original input. Note that we can drop the
denominator $p(S)$ as it will be constant throughout the process.

\begin{equation}
\hat{T} = \argmax_T p(T|S) = \argmax_T \frac{p(S|T)p(T)}{p(S)} = \argmax_T p(S|T)p(T)
\label{eq:introsmt2}
\end{equation}

The two factors we end up with in Equation~\ref{eq:introsmt2} correspond
precisely to the \emph{translation model} ($p(S|T)$) and \emph{language model}
($p(t)$) respectively.

\subsubsection{Translation Model}

The translation model is learned from parallel corpus data. A parallel corpus
has the same text in two or more languages, i.e. the corpora in a parallel corpus are translated versions of one another. Early SMT systems were word-based, i.e.\ the translation model
consisted of probabilities for word translation pairs. These systems have been
superseded by phrase-based systems, in which multiple words at once may be
translated as an entity \citep{PBSMT}. This allows for translations in which
multiple words in the source language translate to just one in the target
language, or vice versa.

The basis for training such a translation model is a parallel corpus, i.e.\ the
same text in two languages. First a \emph{sentence alignment} is computed
\citep{TIEDEMANN}. This identifies sentences that are translations of each other
through statistical means. Subsequently, a \emph{word alignment} is computed,
according to a method pioneered by \cite{BROWN}. See also the work of
\cite{OchNey2003} and \cite{TIEDEMANN}. The word alignment attempts to
identify words that are translations of each other, by means of an
\emph{expectation maximisation} algorithm \citep{EXPMAX}. Finally, we can derive phrase
translation pairs from these word alignments.

The outcome of the training stage is a \emph{phrase translation table}, which
constitutes the translation model. It is a collection of phrase pairs $(s,t)$
in source and respectively target language language. Associated with each pair
are the phrase-translation probabilities $p(s|t)$ and $p(t|s)$, derived from
the joint frequency of the pair and the individual frequencies of the parts in
the data. Additional probabilities, such as lexical weights and word/phrase
penalties, may be included as well.

At test time, the translation probability $P(S|T)$ for a given translation
hypothesis is estimated by computing the product over all phrase pairs that
make up the hypothesis ($s \in S$, $t \in T$). A score is computed for each
based on the product of $p(s|t)$ with optionally $p(t|s)$ and other scoring
components. Each of the components in this \emph{score function}
is typically parametrised by a weight ($\lambda$). This is shown in equation~\ref{eq:introsmt3}.

\begin{equation}
P(S|T) = \prod_{i=1}^{n} p(s_i|t_i)^{\lambda_1} p(t_i|s_i)^{\lambda_2}
\label{eq:introsmt3}
\end{equation}

The value of the weights $\lambda_1$ and $\lambda_2$ for the score function cannot be determined a priori,
but has to be computed experimentally on held-out tuning data. This essentially
comes down to trying a huge variety of parameter values and experimentally
observe which performs best on the data.  This final \emph{parameter
optimisation} step is an essential part of SMT \citep{MERT}.

\subsubsection{Language Model}

Note that the translation model is not sensitive to the order in which
fragments are assembled. Ordering is instead guided by the language model,
the second key component in an SMT system.

The language model predicts the likelihood of a word given a previous history
of predecessor words. This is a model of \emph{context}; the difference with
the context that is the focus in this dissertation, however, is that it models
the target language rather than source language. Moreover, it is a purely
monolingual model whereas our investigation is aimed at modelling source-side
context for translation directly.

A simple trigram-based language model is shown in equation~\ref{eq:introsmtlm1}, adapted from
\cite{Cole1997}, where we compute the probability of a sequence of words $W$.

\begin{equation}
p(W) = p(w_1,\ldots,w_n) = \prod_{i=1}^{n} p(w_i|w_{i-2},w_{i-1})
\label{eq:introsmtlm1}
\end{equation}

An actual language model will likely also implement \emph{back-off} strategies and
\emph{smoothing}, to avoid the problem of encountering words unseen during
training, which would result in a probability score of $0$ otherwise.

The role of the language model is mainly to assign a higher probability to
sentences in which the target-language phrases are ordered properly than
to sentences in which phrases are in an unnatural order.

\subsection{Word Sense Disambiguation}

Word Sense Disambiguation (WSD) shares certain characteristics with
translation. The idea in WSD is to disambiguate between multiple meanings of a
polysemous words given a context. Consider the following example:

\begin{enumerate}
\item ``My money is safely deposited in the \textbf{bank}.''
\item ``The ship avoided crashing into the \textbf{bank} of the river.''
\end{enumerate}

The first example refers to a bank as a financial institution, whereas the
second refers to the boundary of a river. In Word Sense Disambiguation we
attempt to automatically infer which sense is intended. The actual sense is
typically represented in the form of a sort of identifier from a sense inventory
such as WordNet.

We already saw in Section~\ref{sec:bridges} that translation requires a
similar disambiguation. After all, translation usually aims to preserve the
semantics. If we translate ``bank'' from these two examples from English to
Spanish, we will obtain ``banco'' for the first example, but ``orilla'' for the
latter. In Chapter~\ref{chap:clwsd} we will take an in-depth look at using
WSD techniques for localised translation problems.

\section{Research Questions}
\label{sec:overallresearchquestion}

Our intuition is that source-side context information, i.e.\ words in the
source-language from the surrounding context, provides valuable cues for
translation. The examples we have discussed hitherto aim to illustrate this
intuition. The over-arching
question we posit for the dissertation is the following:

\begin{enumerate}
\item To what extent can we improve translation by considering source-side
    context information?
\item What context features prove most effective?
\item To what extent are linguistically uninformed features effective?
\item How can source-side classifiers be used in translation tasks?
\end{enumerate}

Here \emph{translation} is understood to be a generic term that
encompasses the more localised translation of fragments in a
context, comparable to Word Sense Disambiguation, as well as a full integration
of source-side context in Statistical Machine Translation.

In the first chapters, some linguistically-informed features such as lemma
features and part-of-speech tags are tested. Our emphasis in the overall study,
however, is on assessing the efficiency of \emph{linguistically uninformed}
features, i.e.\ employing only the raw textual data. The reason for this is that we
aim to assess the merit of the techniques as such, a pure analysis without
introducing another data dimension. Moreover, by choosing this approach we do
not need to rely on external language-specific tools such as as lemmatisers and
part-of-speech taggers. In subsequent chapters, we will also show that the
inclusion of linguistically-informed features is something already studied and
described in the literature by others.

Our main hypothesis is that inclusion of source-side context information, without
linguistically informed features, benefits translation quality.

\section{Thesis structure}

We begin our research on the level of translating simple words in context, and
gradually move up in complexity and integration with SMT throughout the
dissertation.

{\nobibliography*
Before we can go into the matter of translation, Chapter~\ref{chap:coco} first introduces the software that was needed
to efficiently extract patterns such as n-grams, with their context, from corpus data. This chapter was published in
``{\footnotesize\begin{NoHyper}\bibentry{COLIBRICORE}\end{NoHyper}}''. The software
was explicitly written in the scope of this research project and provides a necessary foundation that is in turn used by most of the
software of the remaining chapters. It is a chapter of a high technical nature that can optionally be skipped if the
reader wishes to more quickly proceed to the heart of the matter. At the same time, the chapter offers tools that are
applicable well beyond the present dissertation.

Chapter~\ref{chap:clwsd} introduces \emph{Cross-Lingual Word Sense
Disambiguation} and presents a classifier-based system employing source-side
context information. This system, when tested against competing systems, obtains the
best or near-best accuracy. This chapter is based on two publications,
``\begin{NoHyper}{\footnotesize\bibentry{WSD1}}'', and ``{\footnotesize\bibentry{WSD2}}\end{NoHyper}''

Chapters~\ref{chap:colibritapilot},~\ref{chap:semeval2014task5}
and~\ref{chap:colibritafinal} are strongly linked and open up a novel line of
investigation. We propose, test and present a translation assistance system
capable of translating fragments in a cross-lingual context, i.e. fragments in
language different from the language of the context. Translation subsequently
proceeds to translate these other-language fragments to the language of the context.
This system changes and investigates the notion of what source-side context is and may
find practical application in Computer Assisted Translation and Computer Aided
Language Learning. In Chapter~\ref{chap:colibritapilot} we describe a pilot study to
assess the feasibility of the idea. This chapter is based on
``{\footnotesize\begin{NoHyper}\bibentry{COLIBRITAPILOT}\end{NoHyper}}''. It applies the lessons learned from
Cross-Lingual Word Sense Disambiguation (Chapter~\ref{chap:clwsd}) to a new
task.  In chapter~\ref{chap:semeval2014task5} we
manually construct a data set and evaluate
the performance of several 3rd party participating systems on our data. This is set up as a shared task for the SemEval
2014 conference and was published as ``{\footnotesize\begin{NoHyper}\bibentry{SEMEVAL2014TASK5}\end{NoHyper}}''.
 In Chapter~\ref{chap:colibritafinal} we finally present our own reworked
``Colibrita'' system on the basis of the pilot study and findings from the
SemEval participants. Here we present the results and form conclusions
regarding the role of context and the usefulness of classifiers versus
Statistical Machine Translation techniques. This chapter also appeared in ``{\footnotesize the International Journal of Translation
(2016), Bahri Publications}'', but was not peer-reviewed.

Chapter~\ref{chap:contextinsmt} is an attempt to directly integrate source-side
context information in Statistical Machine Translation, by means of
memory-based classifiers. It has not been published and is a novel contribution to this research. Chapter~\ref{chap:conclusion} aggregates and
summarizes the conclusions from earlier chapters, and forms a final answer to
our research questions.
}
