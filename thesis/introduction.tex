\chapter{Introduction}

“To know an object is to lead to it through a context which the world provides.” -- William James (American Philospher, 1861-1947)

"Form is emptiness, emptiness is form" -- Heart Sutra

\section{Context as Linguistic Bridges}

\subsection{Context}

To even begin to merit the title of ``Doctor of Philosophy'', it is only proper
to start this dissertation with some philosophical deliberations. This will be
in sharp contrast to more technical nature of the rest of these chapters.

The study you are about to read sprung from the intuition that \emph{context}
is an important, if not the most important, characteristic that defines
language. Language itself only exists only in the context of the world that
surrounds us, as well our internal mental world. Without this context, there is
nothing to talk about in the first place. Language is rightfully considered the
epitome of human evolution. Our species evolved the remarkable ability to refer
to the world outside and within by producing complex meaningful utterances,
i.e.\ speech. 


This was an unparalleled revolution in \emph{communication}, which has
undoubtedly played a leading role in us becoming the dominant intelligent
species on the planet. It has put us in a position where we can communicate our
thoughts and feelings about the world to one another on a more fine-grained
level than any other species can. Moreover, it has given us the ability of ever
increasing \emph{abstraction}. The context of our language is no longer limited
to just refer to objects in our immediate vicinity, but we can even refer to
abstract thought itself. Whenever communication is attempted between people who
do not share a language, the context to which can be referred is dramatically
restricted.  Just imagine tourists in a foreign bakery lustfully pointing at
the pastries they desire. 

Another major revolution in communication has been the development of writing,
and much later that of print technology. This and the accompanied literacy of
populations allows for our thoughts and ideas to be preserved more easily and
accurately than oral traditions can accomplish. The ability to read and write
broadens one's world, one's context, and is therefore even deemed a fundamental
human right. 

Language is inherently ambiguous and context is the disambiguating factor
without which it can not exist. The context of a bakery and a neatly arranged
line of pastries is essential for the baker to be able to disambiguate the
pointing of the clueless foreign tourist and discern which pastry he actually
wants. Demonstrative pronouns such as ``he'', ``she'', ``this'', or even
definite noun phrases such as ``the house'' convey little information if not
for the context they're employed in.  In fact, any word by itself is pretty
limited, can never exist in total isolation, and only derives meaning from the
further context. It is even defined purely through the context, just like a
dictionary defines words in terms of other words.

%If the context of your own
%upbringing was Indian and you had been brought up learning how to read
%devanagari script, you would probably be able to read that as ``shunyata''.
%Merely being able to read and vocalize the word still wouldn't bring you closer
%to its meaning though. 
%The word is Sanskrit, though it comes from a Pali word,
%as is the form in which it is written in an Buddhist scripture called the
%``Heart Sutra''. The fact we can can even still talk about this word and its usage in a
%2500-year old text today is already a testimony to the major impact writing
%technology has on our civilization. Hundreds of millions of adherents of
%various major religions still derive their daily religious practice from
%throughts and ideas put down in writing in ancient times. Moreover, as a
%scientific community, we base our studies on the work of our predecessors,
%explicitly citing them as our methodology requires. But I digress, as my
%main point relates to the meaning of this word. 

Buddhist philosophy has a concept called \emph{``sh\=unyat\=a''}. Sh\=unyat\=a is Sanskrit and can
very roughly be translated into English as ``emptiness''. It  refers to the
idea that everything, not just words, is interrelated and nothing has any
existence or essence of or by itself: this philosophy posits that everything
exists and is defined purely through \emph{its context} and can by definition
never be seen as independent from it.

\subsection{Linguistic Bridges}

Whereas we humans all share an astounding capacity for language, history has
given rise to numerous distinct and often mutually unintelligable languages.
This bring us back to the predicament of our pastry-loving tourist in the
foreign bakery, unable to express his choice in the language of the baker and
therefore resorting to pointing. If we situate the bakery in France, the baker
may reply ``croissant'' in response to the gesturing of his client. The
perceptive client may then have learned that this is what the pastry is called.
The famous utterance ``Me Tarzan, You Jane'', heavily supported by gestures as
in the classic movie ``The Ape Man'', is of a similar nature. Both protagonists
breach the language barrier and gain new information. These example show that
context plays an important role in establishing translation, or any common
vocabulary. 

The \emph{linguistic bridges} of this dissertation refer to these acts of
translation. We can define translation as \emph{a process that yields a
representation of a message initially expressed in one language, in another}.
Etymologically, the word \emph{translation}, from the latin \emph{transl\=ati\=o},
refers to carrying (l\=ati\=o) accross (trans) something. The focus is generally on
accurate preservation of the semantics of message.  Although in some arts, such
as poetry, form or emotion may take presendence over the substance.

The intuition underlying our research is that the context of word or phrase is
an important cue for the translation of that word or phrase. A word in context
A, may be translated differently than the same word in context B. Consider the
Portuguese word ``tempo'' in the following sentences:

\begin{enumerate}
\item ``O \textbf{tempo} é bom hoje.'' -- ``The \textbf{weather} is nice today.''
\item ``Não tenho \textbf{tempo} para estudar hoje.'' -- ``I don't have \textbf{time} to study today.''
\end{enumerate}

In the first sentence, ``tempo'' is translated as ``weather'', whereas in the
second it is translated as ``time''. Portuguese uses the same word where
English uses two distinct words; context provides cues to disambiguate into the
proper translation. Another example shows where English has one word and French has two:

\begin{enumerate}
\item ``J'ai tué \textbf{la mouche}'' -- ``I killed \textbf{the fly}''
\item ``Je \textbf{vole} à Paris'' - ``I \textbf{fly} to Paris''
\item ``Je \textbf{vole} l'horloge de mon père'' -- ``I steal my father's watch''
\end{enumerate}

In this latter example, disambiguation between the first two sentences is
facilitated by the fact that the word \emph{fly} has a different
part-of-speech in both sentences, unlike the noun \emph{tempo} in the earlier example.
The presence of the article ``the'' in the noun phrase ``the fly'' already
rules out a translation to a verb. The French verb ``voler'' in turn does not
just translate to ``to fly'', but can also mean ``to steal'', as shown in
the last sentence. 

These examples illustrate that languages are almost never translatable on a naive
word-by-word basis, and that context plays an major role in determining the
right translation. Context plays a bridging function in translation, and this
dissertation investigates techniques for machines to exploit this to attain
better automatic translations.

\section{Natural Language Processing}

In our philosophical deliberations we briefly addressed the communication
revolutions brought about by humanity evolving the faculty of language and
speech, and later the technological development of writing and print. We all
live in fortunate times to have just witnessed the biggest revolution in
communication since the invention of print: the information revolution. Text is
not just printed anymore, it is digitized; i.e.\ made available, distributable,
and searchable in digital form. The internet enables these digital resources to
be collected, shared, and exploited at unpredecented rate and allows us to
communicate instantaneously with people from all over the planet. 

%To emphasize the importance of this development, and to underline my personal
%conviction that this is the way the future should go; this dissertation itself
%will be made available primarily in an open digital form, rather than printed
%book form.

This wealth of digitized data fuels our field of research: Natural Language
Processing (NLP). We attempt to extract meaningful data from natural language.
Most contemporary techniques in NLP are grounded in Machine Learning, which
employs statistical principles to learn whatever patterns from big collections
of natural language data. These data-driven approaches in NLP can be contrasted
to the more rule-based approaches, not without their own merit, that are
directly derived from human expert knowledge.

This dissertation strictly follows the data-driven trend and we therefore rely
on large amounts of digitized text to conduct our research. In this section we
give some theoretical background on \emph{Machine Translation} and \emph{Word
Sense Disambiguation}, in order to be able to understand the subject matter of
this dissertation.

\subsection{Machine Translation}

The area of research dedicated to the automatic translation of data from one
natural language to another is called \emph{Machine Translation} (MT).
State-of-the-art techniques in this field are primarily based on statistical
methods, in which case we can speak of \emph{Statistical Machine Translation}
(SMT). These can contrasted to earlier rule-based Machine Translation systems.
These classical methods each have an \emph{analysis} stage, a \emph{transfer}
stage according to a set of rules, and a \emph{generation} stage. Such transfer may occur on a shallow, a syntactic or a semantic level. 

Statistical Machine Translation treats translation as a problem where an
ordered sequence of words or phrases has to be found that is the most probable
translation of the input sentence. It effectively tests many possible
translation for individual fragments, and various ways in which these
translated fragments can be recombined. Each test is a \emph{translation
hypothesis}, and thousands or of such hypotheses are evaluated by the system
for a single sentence. The one with the highest probability score is eventually
selected.

Statistical Machine Translation is an integrated solution that handles two
problems at once; that of finding the best lexical translation for word or
phrases, and that of reassembling those in a natural order. This corresponds to
two key properties of translation:

\begin{enumerate}
	\item \textbf{Semantic faithfulness} - A translation must accurately transfer meaning for all of its parts. The right lexical translation has to be found for the words and expressions in the source.
	\item \textbf{Syntactic fluency} - The translated result must be fluently expressed, the sentence must have proper syntax..
\end{enumerate}

In Statistical Machine Translation, two key models tackle these issues. The
\emph{translation model} models the translation of words or phrases from the
source language to the target language. The \emph{language model}, for the
target language, provides the means to reassemble translated fragments. Both
are probablistic models that are learned from data. The translation model
requires a \emph{parallel corpus} as input, and the language model can be
trained on monolingual data.

As said, the act of translation is expressed as the search for the most
probable translation. Take $S$ to be a sentence in the source language, and $T$
to be a translated sentence, i.e.\ a translation hypothesis, in the target
language. The SMT process can then be formalised as in
Equation~\ref{eq:introsmt1}, where the outcome $\^T$ corresponds to the most probable translation.

\begin{equation}
\^T = \argmax_T p(T|S) 
\label{eq:introsmt1}
\end{equation}

By applying Bayes' theorem we can invert this. The denominator can be dropped
as it is the same throughout the search. This inversion is necessary as we can
not model $p{T|S}$ directly. Instead, we apply what is known as the \emph{noisy
channel model}. This is formalised in Equation~\ref{eq:introsmt2}. The system
generates a huge amount of translation hypotheses. Interpret this as if our
original sentence in the source language entered a noisy channel and comes out
scrambled in the target language at the other end. We then compute how probable
a translation it is of the original input. Note that we can drop the
denominator $p(S)$ as it will the constant throughout the process.

\begin{equation}
\^T = \argmax_T p(T|S) = \argmax_T \frac{p(S|T)p(T)}{p(S)} = \argmax_T p(S|T)p(T)
\label{eq:introsmt2}
\end{equation}

The two factors we end up with in Equation~\ref{eq:introsmt2} correspond
precisely to the \emph{translation model} and \emph{language model} respectively.

\subsubsection{Translation Model}

The translation model is learned from parallel corpus data. Early SMT systems
were word-based, i.e.\ the translation model consisted of probabilities for
word translation pairs. These systems have been superseded by phrase-based
systems, in which multiple words at once may be translated as an entity
\citep{PBSMT}. This allows for translations in which multiple words in the
source language translate to just one in the target language, or vice versa.

The basis for training such a translation model is a parallel corpus, i.e.\ the
same text in two languages. First a \emph{sentence alignment} is computed
\citep{TIEDEMANN}. This identifies sentences that are translations of eachother
through statistical means. Subsequently, a \emph{word alignment} is computed,
according to a method pioneered by \cite{BROWN}. See also the work of
\cite{OchNey2003} and \citep{TIEDEMANN}. The word alignment attempts to
identify words that are translations of eachother, by means of a
\emph{expectation maximisation} algorithm. Finally, we can derive phrase
translation pairs from these word alignments, using algorithms such as
\emph{grow-diag-final} as described in \citep{PBSMT}. 

We outcome of the training stage is a \emph{phrase translation table}, which
constitutes the translation model. It is a a collection of phrase pairs $(s,t)$
in source and respectively target language language. Associated with each pair
are the phrase-translation probabilities $p(s|t)$ and $p(t|s)$, derived from
the joint frequency of the pair and the invididual frequencies of the parts in
the data. Additional probabilities, such as lexical weights and word/phrase
penalties, may be included as well. 

At test time, the translation probability $P(S|T)$ for a given translation
hypothesis is estimated by computing the product over all phrase pairs that
make up the hypothesis ($s \in S$, $t \in T$). A score is computed for each
based on the product of $p(s|t)$ with optionally $p(t|s)$ and other scoring
components. Each of the components in this \emph{score function} 
is typically parametrised by a weight ($\lambda$). This is shown in equation~\ref{eq:introsmt3}.

\begin{equation}
P(S|T) = \prod_{i=1}{n} \lambda_1\p(s_i|t_i)\lambda_2\p(t_i|s_i)
\label{eq:introsmt3}
\end{equation}

The value of the weights for the score function can not be determined a priori,
but has to be computed experimentally on held-out tuning data. This essentially
comes down to trying a huge variety of parameter values and experimentally
observe which performs best on the data.  This final\emph{parameter
optimisation} step is an essential part of SMT \citep{MERT}.

\subsubsection{Language Model}

Note that the translation model is not sensitive to the order in which
fragments are assembled. Ordering is instead directed by the language model,
the second key component in an SMT system.

The language model predicts the likelihood of a word given a previous history
of predecessor words. This is a model of \emph{context}; the difference with
the context that is the focus in this dissertation, however, is that it models
the target language rather than source language. Moreover, it is a purely
monolingual model whereas our investigation is aimed at modelling source-side
context for translation directly.

A simple trigram-based language model is shown in equation~\ref{eq:intosmtlm1}, adapted from
\cite{Cole1997}, where we compute the probability of a sequence of words $W$.

\begin{equation}
p(W) = p(w_1,\ldots,w_n) = \prod_{i=1}{n} p(w_i|w_{i-2},w_{i-1})
\label{eq:introsmtlm1}
\end{equation}

An actual language model will likely also implement \emph{back-off} strategies and
\emph{smoothing}, to avoid the problem of encountering words unseen during
training, which would result in a probability score of $0$ otherwise. 

The role of the language model is mainly to assign a higher probability to
sentences in which the target-language phrases are ordered properly than
phrases in an unnatural order. 


\subsection{Word Sense Disambiguation}


\section{Research Question}


\section{Thesis structure}























