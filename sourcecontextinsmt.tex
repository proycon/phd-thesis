\documentclass[11pt]{article}
%\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{framed}
\usepackage{pbox}
\usepackage{supertabular}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{gb4e}

\title{Classifier-based modelling of source-side context information for Statistical Machine Translation}


\author{Maarten van Gompel \& Antal van den Bosch \\
 Centre for Language Studies \\
  Radboud University Nijmegen \\
  {\tt proycon@anaproy.nl}}

%\date{}


\begin{document}
\maketitle

\begin{abstract} 
We present in-depth research into the modelling of source-side
context to improve Phrase-based Statistical Machine Translation. Statistical
Machine Translation systems typically consist of a translation model and a
language model. The former maps phrases in the source language to the target
language, without regard for the context in which the source phrases occur. The
latter models just the target language, and acts as a target-side model of
context information after translation. We attempt to independently reproduce a
line of existing research and test whether considering context information
directly in the translation model has a positive effect on translation quality.
We furthermore investigate various ways classifier-based models can be
integrated into Statical Machine Translation.  We will use proven techniques
from Word Sense Disambiguation, effectively integrating WSD techniques in
Statistical Machine Translation. Our approach is classifier-based and our focus
is exclusively unsupervised, we therefore do not include any additional linguistic
features. 
\end{abstract}

\section{Introduction}

In Phrase-based Statistical Machine Translation (SMT) the problem of
translating an input sentence from a source language to a target language is
perceived as a game of probabilities and a search for the most probable
translation option.  These probabilities are expressed in a number of models
that specialize in a certain aspect relevant to the translation process. The
``phrase-based'' characteristic is due to phrases being the building blocks of
the translation model. This can be contrasted to earlier approaches in
Statistical Machine Translation which started as word-based \cite{OCHNEY?}.
Phrases in this sense have to be perceived simply one or more words, i.e.
n-grams of variable length (including unigrams). Moreover, they are not at all
required to form a proper linguistic constituent of any kind.

Two models are at the core of phrase-based SMT: first there is the translation
model which maps the translation phrases in the source language ($s$) to
phrases in target language ($t$), this mapping is expressed as a vector of
probabilities, most notably $P({phrase}_s|{phrase}_t)$ and
$P({phrase}_t|{phrase}_s)$. This component can be seen to model the notion of
``semantic faithfullness''; if you translate a phrase from one language to
another, you want the meaning to be preserved as accurately as possible. The
second core model is the language model, this model in monolingual in nature
and models \emph{the target language}. It models what words are likely to
follow others and can be interpreted as modelling the ``syntactic fluency''
notion of translation; a translation should be in a natural word-order and
sound natural. A Machine Translation \emph{decoder} optimises a log-linear
model of these two, and additional other, models. Given an input sentence in
the source language, it searches through a vast space of all ``possible''
translation options, most non-sensical, for a path maximising the probabilities
according to each of the models, taking into account different weights they may
be assigned.

The study we currently present focusses on the role of surface-form context
information in this SMT process. The Language Model effectively models context
for the target language, it makes sure that a translated phrase fits nicely
along other translated phrases. But in SMT there is no component modelling
context for the source-language, whereas intuitively source-side context may
provide a powerful cue for translation. Consider the word ``bank'' and its
Spanish translation in examples~\ref{ex:bank1} and \ref{ex:bank2}.

\begin{exe} %gb4e package
\ex \textbf{English:} I don't trust the bank. \\
    \textbf{Spanish:} No me fio del banco.
\label{ex:bank1}

\ex \textbf{English:} The boat headed towards the bank of the river. \\
    \textbf{Spanish:} El barco se dirigió hacia la orilla del río.
\label{ex:bank2}
\end{exe}

The same English word, ``bank'' may express multiple semantic senses, some of
which are expressed by different words in Spanish. Source-side context
information may provide valuable clues to what sense is being employed, and
therefore what translation is correct.  The words ``boat'' and the phrase ``of
the river'' in example \ref{ex:bank2} make it pretty clear that we are using
bank in its maritime sense. Example \ref{ex:bank1} is less obvious, but the
word ``trust'' could be seen to be a cue when the noun ``bank'' denotes a
financial institution.

These examples are meant to illustrate the intuition that is behind our
research hypothesis. We hypothesise that the inclusion of source-side context
information in the translation model improves translation results, as the
context helps in providing a more accurate disambiguation. A counter-hypothesis
to this would be that whilst source-side context information is not modelled
explicitly, it is implicitly captured by the combination of translation model
and language model, and explicit modelling has no added value.

There is clear and obvious overlap between what we do here and the field of
Word Sense Disambiguation (WSD). We effectively test an integration of proven
techniques from WSD in Statistical Machine Translation, and apply these to
phrases rather than just words.

WSD systems often employ a variety of linguistic features. The focus of our
study, however, is fully unsupervised. We are interested only in the surface
forms, the text as-is, and stay as close as possible to vanilla Phrase-Based
Statistical Machine Translation, without using any language-specific external
resources. In this fashion, we attempt to assess the merit of source-side
context information as it is in its unmodified form. 

Not introducing extra data for the translation system means our goals have to
be set more modest as well. We do not expect as much gains as 


\section{Previous research}

The idea to integrate WSD approaches in SMT is not new, nor is the idea to use
source-side context information to disambiguate in translation. Various studies
have been conducted with mixed results. In the early days of SMT,
\cite{GarciaVarea+02} already explicitly modelled source-side context in a
maximum entropy model for word-based SMT, and report slightly improved error
rates on a translation task.

\cite{CarpuatWu05} were the first to directly tackle the question whether
full-scale WSD models were beneficial to translation quality when integrated in
SMT systems, and thus their work forms important foundation for our own study.
Their approach uses an ensemble classification model that integrates
position-sensitive, syntactic, and local collocational features, which has
proven itself in competitive WSD tasks. This includes linguistic features such
as part-of-speech tags and lemmas, as well as more complex syntactic relations.
They test for a single Chinese-to-English test-set only, and only use BLEU,
which raises some questions on whether their conclusions would hold on
different language pairs, test sets, and using different evaluation metrics.

\cite{CarpuatWu05} place strong focus on the WSD model rather than the SMT
model, whereas we place more focus on the SMT model and the integration method,
and keep the ``WSD-model'' relatively simple. Furthermore, the method they
employ a simpler word-based form of Statistical Machine Translation and the
level of integration seems limited.

Despite their efforts, they reach the surprising conclusion that inclusion of
WSD techniques does \emph{not} yield better translation quality. Will these
results hold in a more modern Phrase-based Statistical Machine Translation
approach?

Two years later they expanded their study to full phrasal units
\citep{CarpuatWu07} and, for the first time, found results that did support the
hypothesis that SMT benefits from the integration of WSD techniques. They now
focus on better integration in \emph{phrase-based} SMT: ``Rather than using a
generic SenseEval model as we did in \cite{CarpuatWu05}, here both the WSD
training and the WSD predictions are integrated into the phrase-based SMT
framework.'' \citep{CarpuatWu07}. They also broaden their use of evaluation
metrics, yet still test on only Chinese to English.

Another influential study 


The most important and complete study we build upon is \cite{Rejwanul+11}. The
main difference is that they focus on a variety of linguistically-informed
contextual features, whereas we focus purely on unsupervised local context.
Their study finds that including linguistically-informed contextual features in
general produces improvements.

important.

In
their work, \cite{Rejwanul+11} focus on a variety of contextual features.

, and effectively attempt to reproduce to a large degree, is \cite{}.



We will use memory-based classifiers to model source-side context, this too has
been done in previous studies.

this has been done in previous studies as well. We closely reproduced techniques used by 







Our intention here is to settle some of the conflicting reports and attempt to independently come to a conclusion regarding the incorporation of source-side context in SMT.



\end{document}
