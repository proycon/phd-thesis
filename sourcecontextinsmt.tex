\documentclass[11pt]{article}
%\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{framed}
\usepackage{pbox}
\usepackage{supertabular}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{gb4e}

\title{Classifier-based modelling of source-side context information for Statistical Machine Translation}


\author{Maarten van Gompel \& Antal van den Bosch \\
 Centre for Language Studies \\
  Radboud University Nijmegen \\
  {\tt proycon@anaproy.nl}}

%\date{}


\begin{document}
\maketitle

\begin{abstract} 
We present in-depth research into the modelling of source-side
context to improve Phrase-based Statistical Machine Translation. Statistical
Machine Translation systems typically consist of a translation model and a
language model. The former maps phrases in the source language to the target
language, without regard for the context in which the source phrases occur. The
latter models just the target language, and acts as a target-side model of
context information after translation. We attempt to independently reproduce a
line of existing research and test whether considering context information
directly in the translation model has a positive effect on translation quality.
We furthermore investigate various ways classifier-based models can be
integrated into Statical Machine Translation.  We will use proven techniques
from Word Sense Disambiguation, effectively integrating WSD techniques in
Statistical Machine Translation. Our approach is classifier-based and our focus
is exclusively unsupervised, we therefore do not include any additional linguistic
features. 
\end{abstract}

\section{Introduction}

In Phrase-based Statistical Machine Translation (SMT) the problem of
translating an input sentence from a source language to a target language is
perceived as a game of probabilities and a search for the most probable
translation option.  These probabilities are expressed in a number of models
that specialize in a certain aspect relevant to the translation process. The
``phrase-based'' characteristic is due to phrases being the building blocks of
the translation model. This can be contrasted to earlier approaches in
Statistical Machine Translation which started as word-based \cite{OCHNEY?}.
Phrases in this sense have to be perceived simply one or more words, i.e.
n-grams of variable length (including unigrams). Moreover, they are not at all
required to form a proper linguistic constituent of any kind.

Two models are at the core of phrase-based SMT: first there is the translation
model which maps the translation phrases in the source language ($s$) to
phrases in target language ($t$), this mapping is expressed as a vector of
probabilities, most notably $P({phrase}_s|{phrase}_t)$ and
$P({phrase}_t|{phrase}_s)$. This component can be seen to model the notion of
``semantic faithfullness''; if you translate a phrase from one language to
another, you want the meaning to be preserved as accurately as possible. The
second core model is the language model, this model in monolingual in nature
and models \emph{the target language}. It models what words are likely to
follow others and can be interpreted as modelling the ``syntactic fluency''
notion of translation; a translation should be in a natural word-order and
sound natural. A Machine Translation \emph{decoder} optimises a log-linear
model of these two, and additional other, models. Given an input sentence in
the source language, it searches through a vast space of all ``possible''
translation options, most non-sensical, for a path maximising the probabilities
according to each of the models, taking into account different weights they may
be assigned.

The study we currently present focusses on the role of surface-form context
information in this SMT process. The Language Model effectively models context
for the target language, it makes sure that a translated phrase fits nicely
along other translated phrases. But in SMT there is no component modelling
context for the source-language, whereas intuitively source-side context may
provide a powerful cue for translation. Consider the word ``bank'' and its
Spanish translation in examples~\ref{ex:bank1} and \ref{ex:bank2}.

\begin{exe} %gb4e package
\ex \textbf{English:} I don't trust the bank. \\
    \textbf{Spanish:} No me fio del banco.
\label{ex:bank1}

\ex \textbf{English:} The boat headed towards the bank of the river. \\
    \textbf{Spanish:} El barco se dirigió hacia la orilla del río.
\label{ex:bank2}
\end{exe}

The same English word, ``bank'' may express multiple semantic senses, some of
which are expressed by different words in Spanish. Source-side context
information may provide valuable clues to what sense is being employed, and
therefore what translation is correct.  The words ``boat'' and the phrase ``of
the river'' in example \ref{ex:bank2} make it pretty clear that we are using
bank in its maritime sense. Example \ref{ex:bank1} is less obvious, but the
word ``trust'' could be seen to be a cue when the noun ``bank'' denotes a
financial institution.

These examples are meant to illustrate the intuition that is behind our
research hypothesis. We hypothesise that the inclusion of source-side context
information in the translation model improves translation results, as the
context helps in providing a more accurate disambiguation. A counter-hypothesis
to this would be that whilst source-side context information is not modelled
explicitly, it is implicitly captured by the combination of translation model
and language model, and explicit modelling has no added value.

There is clear and obvious overlap between what we do here and the field of
Word Sense Disambiguation (WSD). We effectively test an integration of proven
techniques from WSD in Statistical Machine Translation, and apply these to
phrases rather than just words.

WSD systems often employ a variety of linguistic features. The focus of our
study, however, is fully unsupervised. We are interested only in the surface
forms, the text as-is, and stay as close as possible to vanilla Phrase-Based
Statistical Machine Translation, without using any language-specific external
resources. In this fashion, we attempt to assess the merit of source-side
context information as it is in its unmodified form. 

Not introducing extra data for the translation system means our goals have to
be set more modest as well. We can not expect the same gains as are achieved by
introducing extra data from supervised sources.

%TODO: write how paper is structured

\section{Previous research}

The idea to integrate WSD approaches in SMT is not new, nor is the idea to use
source-side context information to disambiguate in translation. Various studies
have been conducted with mixed results. In the early days of SMT,
\cite{GarciaVarea+02} already explicitly modelled source-side context in a
maximum entropy model for word-based SMT, and report slightly improved error
rates on a translation task.

\cite{CarpuatWu05} were the first to directly tackle the question whether
full-scale WSD models were beneficial to translation quality when integrated in
SMT systems, and thus their work forms important foundation for our own study.
Their approach uses an ensemble classification model that integrates
position-sensitive, syntactic, and local collocational features, which has
proven itself in competitive WSD tasks. This includes linguistic features such
as part-of-speech tags and lemmas, as well as more complex syntactic relations.
They test for a single Chinese-to-English test-set only, and only use BLEU,
which raises some questions on whether their conclusions would hold on
different language pairs, test sets, and using different evaluation metrics.

\cite{CarpuatWu05} place strong focus on the WSD model rather than the SMT
model, whereas we place more focus on the SMT model and the integration method,
and keep the ``WSD-model'' relatively simple. Furthermore, the method they
employ a simpler word-based form of Statistical Machine Translation and the
level of integration seems limited.

Despite their efforts, they reach the surprising conclusion that inclusion of
WSD techniques does \emph{not} yield better translation quality. Will these
results hold in a more modern Phrase-based Statistical Machine Translation
approach?

Two years later they expanded their study to full phrasal units
\citep{CarpuatWu07} and, for the first time, found results that did support the
hypothesis that SMT benefits from the integration of WSD techniques. They now
focus on better integration in \emph{phrase-based} SMT: ``Rather than using a
generic SenseEval model as we did in \cite{CarpuatWu05}, here both the WSD
training and the WSD predictions are integrated into the phrase-based SMT
framework.'' \citep{CarpuatWu07}. They also broaden their use of evaluation
metrics, yet still test on only Chinese to English.

The work of \cite{Gimenez+07} is similar, they use support vector machines to
predict the phrase translation probabilities for the phrase-translation table
component of SMT, rather than relying on the context-unaware Maximum Likehood
Estimate the statistical process produces. The feature vector for their
classifiers consists of both local context as well as global context features.
In addition to the surface forms of the words, they do rely on shallow
linguistic features such as Part-of-Speech tags and lemmas. They conduct a
manual evaluation judged on fluency and adequacy, and conclude that considering
context improves adequacy, yet does not benefit fluency. They remark that the
integration of the classifier probabilities in an SMT framework needs further
study, which is something that will indeed be a focus in our present study.

The year 2007 saw a culmination of various studies integrating WSD techiques in
SMT using classifiers. A third study in this trend was \cite{Stroppa+07}. They
have a strong focus on the word form, as does this present study, and add only
part-of-speech features. On IWSLT 2006 data for Chinese-English and
Italian-English, they achieve a significant improvement for the former, whereas
the BLEU score for the latter fails to pass the significance test. We will
attempt to reproduce these experiments in this study.

Source-context aware translation has also been attempted outside of the
predominant statistical machine translation framework. \cite{MBMT} implement a
simple form of example-based machine translation that is word-based and relies
chiefly on classifiers for the translation model component. Two studies derive
from the same concept while transcending a word-based paradigm:
\cite{MARKERBASED} use chunks delimited by common markers. and \cite{PBMBMT}
attempts a full extension to phrases similar to SMT. Although positive results
are achieved in the latter study, it does not rival state-of-the-art SMT.

The most important and complete study we build upon is \cite{Rejwanul+11},
which in turn draws from the majority of the aforementioned studies, and
provides an extensive comparison between them. Their study finds that including
such linguistically-informed contextual features in general produces
improvements.  The main contrast between our study and theirs is that they
focus on a variety of linguistically-informed contextual features, whereas we
depart from a purely unsupervised angle and intend to settle some of the
conflicting reports whether this may lead to an improvement in translation
quality. A notable focus in our study will be possible methods of integrating the
classifier probabilities in the SMT, as recommended also by \cite{Gimenez+07}.


\section{Methodology}
\label{ref:methodology}

Like most of the latest studies before us, we approach the machine translation
problem in a phrase-based fashion, which has superseded the simpler word-based
based paradigm for quite some time. This means that phrases, defined as a
sequence of one or more words (that need not form a linguistic entity in any
way!), form the basic building blocks of our translation model. The problem of
translating a sentence is decomposed into the problem of translating its phrasal
subparts and combining the results in the best order.

In describing our methodology, we first focus on the problem of phrasal
translation, adding in the source-side context component. This shall be done
using classifiers. Then we address how this can be integrated into a
phrase-based Stastical Machine Translation decoder, which also takes care of
ordering aspect. 


\subsection{Modelling source-side context with classifiers}

In line with several previous studies \cite{Rejwanul+11,PBMBMT,
Stroppa+07,MARKEDBASED}, we make use of memory-based classifiers to build a
translation model informed by source-side context information. More
specifically, we will be using IB1 \cite{IB1}, an implementation of k-Nearest
Neighbours; IGTree \cite{IGTree}, an optimised and lossless tree-based approximation thereof;
and TRIBL2, a mixture model of the two. 

These algorithms are all implemented in the
software Timbl \cite{TIMBL}\footnote{\url{http://ilk.uvt.nl/timbl}} and
are well-suited for symbolic data and highly multivariate classes.
Moreover, memory-based classification has been a proven solution in the field
of Word Sense Disambiguation \cite{SENSEVAL2,WSD2}.

When speaking of the $k$ nearest neighbours in the implementation of IB1,
IGTree and TRIBL2, we are actually referring to the neighbours at nearest
distance $k$. So even with $k=1$ we may be talking about multiple data points
that are all at equal distance.

%(these two paragraphs are paraphrased from my PBMBMT thesis, not sure to cite
%or prevent for risk of over-self-citation here)
IGTree compresses the instance base into an ordered decision tree structure at
training time, and issues look-ups in this tree structure at test time. Unlike
other top-down induced decision tree methods such as C4.5, features in IGTree
are tested in a fixed order. This is computed \emph{only once and in advance}
for all features. This order is determined using metrics such as
\emph{information gain} or \emph{gain ratio}. They determine the relative
informativeness or disambiguating power of the feature and provide a ranking of
all features. 

IGTree's performance relies on the differences in information gain between
features. If these are small then IGTree may perform significantly below IB1
\citep{TIMBL}. A hybrid approach called TRIBL2 \citep{TIMBL} starts out with
IGTree and switches to a IB1 variant when a value mismatch occurs in the tree.
In this study, we therefore opt to use TRIBL2 over plain IGTree, but only when
using IB1 would have a prohibitively large impact on performance.

In our classifier-based translation model we will be modelling the probability
of a target phrase ($t$) given a source phrase ($s$) and context information
($c$). We can thus express this as $P(t|s,c)$.  \cite{Stroppa+07} state that
direct estimation of this probability using relative frequencies would result
in overestimation of large phrases, and that therefore a smoothing factor is
required. They proceed to say that through memory-based classification we
implicitly introduce precisely such a smoothing factor.

Given a source phrase and context information, the classifier yields classes
corresponding to target phrases, with an associated weight. After
normalization, these can be considered a posterior distribution of
target phrases. 

We primarily focus on the modelling of local context, i.e. words in the
immediate vicitinity of the source phrase. Take $w_0$ to be the first word of
the source phrase $s$, then for a local context size of $n$ words to the left and
$m$ to the right we construct the feature vector as follows:

\begin{equation}
  \langle w_{-n} .. w_{-1} , s , w_{|s|+1} .. w_{|s|+m} \rangle
\end{equation}

Now there are two ways in which we can construct a classifier:

\begin{itemize}
  \item \textbf{Monolithic classifier} -- One aggregated classifier for all
    source phrases.
  \item \textbf{Classifier experts} -- One classifier per source phrase.
\end{itemize}

In this study we will use and compare both methods, which is, for the task at
hand, the first such a comparison in the literature as far as we know.

For the monolithic classifier, the first feature in the ranking will always be
$s$. Nevertheless, it is quite conceivable that a match for the context is not
found and the classifier proceeds to match on another feature. To prevent
situations in where the classifier falls back to a completely different source
phrase, and thus comes up with unrelated translation options, we enforce that
the source phrases need to match exactly, which is what \cite{Stroppa+07} do as
well.

For the classifier experts, on the other hand, the source phrase is the least
powerful feature in the ranking, as it is shared amongst all instances. We
therefore simply omit it from the feature vector.

\subsection{Training}

The translation model is trained on parallel corpus data. We follow a common
MT pipeline and at the end derive classifier training data.

Given an a tokenised and sentence-aligned parallel corpus, we iteratively learn
word alignments using GIZA++ \citep{GIZA}. Then we identify and extract phrase
pairs using the {\em grow-diag-final}\/ algorithm \citep{OchNey2003}. The
result is a phrase-translation table mapping source phrases to target
phrases, along with associates scores which we will discuss in the next
section. This phrase-translation table effectively constitutes the translation
model.

The translation model would be finished if we would want to leave it to be
non-context-informed. We have some additional steps to perform to train our
context-informed classifiers. We take the phrase-translation table, along with
the parallel corpus, as a basis for extracting training instances.

We build indexed pattern models of all source phrases and target phrases that
occur in both their respective side the parallel corpus, as well as the
phrase-translation table. An indexed pattern model maps each distinct phrase to
the locations in the corpus where it occurs.  Additionaly, a reverse index is
included in the model for the target-side of the corpus, which maps any given
$(sentence, token)$ position to a set phrases that begins at that position.
This is computed using the software \emph{colibri-core}
\footnote{http://proycon.github.io/colibri-core}, which takes care of a
losslessly compressed in-memory representation for all phrases, and allows us
to cope with large corpora.

Given these two pattern models $M_{source}$ and $M_{target}$ we can quickly and
efficiently extract the context for each phrase pair, as shown in simplified
form in Algorithm~\ref{alg:featureextra}. Take $n$ again to be the left
context, $m$ to be the size of the right context, and $w_{(i,j)}$ to denote the
word in the source corpus in sentence $i$, token $j$. The algorithm will return
a list containing two-tuples $(feature vector, translation)$.

\begin{lstlisting}
instances = []
for (s \elem M\_{source}, t \elem M\_{target}):
  for i in sourceindex(s) \cap targetindex(t):
      j = getsourcetoken(s, i)
      features = \langle w\_{i,j-n} \ldots w\_{i,j-1}, s, w\_{i,j+|s|+1} \ldots
      w\_{i,j+|s|+m} 
      instances.append( (features, t) ) 
return instances
\end{lstlisting}
    
This algorithm is implemented in \emph{colibri-mt}
\footnote{https://github.com/proycon/colibri-mt}.

The return instances can be stored directly, either in a single model for the
monolithic approach or in separate models for each $s$ for the classifier
expert approach. A final training phase then computes the feature ranking and
transforms this data into the instance base format required for Timbl.

When extra training data for the classifier(s), it may well happen that either
1) an $(s,t)$ pair only occurs once, or 2) a pattern $s$ occurs in multiple
context but all map to the same $t$. In such cases, a context-informed
classifier clearly has no added value and therefore such instances are omitted from the training data.  

\subsection{Integration in an SMT Decoder}

%TODO: mention discriminative translation filtering (Rejwanul)

The task of an SMT decoder is to find the best translation amongst a vast pool
of possible translation hypotheses. The best translation hypothesis is the
translation hypothesis that maximises a log linear combination and is sought
after in a beam-search algorithm. This log-linear combination draws from
various models, such as a translation model (i.e. the phrase-translation
table), a target-language model optionally additional models such as a
distortion model or word-reordering model.

The translation model is a mapping of the set of source phrases ($S$) to the
set of target phrases ($T$). Each phrase-pair $(s,t)$ where $s \elem S$ and $t
\elem T$ is in described by a score vector indicating the likelihood of
translation. This score vector most notably consists of the probababilities
$p(s|t)$ and $p(t|s)$. In addition, lexical weighting probabilities $lex(s|t)$
and $lex(t|s)$ express the probability of a phrase-pair word-by-word, and are
often included as components in the score vector. During decoding, the total
score of the translation model and other models is expressed as a log-linear
combination, in which different weights can be assigned to each of the
components of the score vector. These weights are parameters to the task and
are typically optimised automatically on development data using for instance
Minimum Error Rate Reduction \cite{MERT}.


For the modelling source-side context the component of the score
vector that 


The state-of-the art SMT decoder used in a majority of MT studies is Moses
\citep{MOSES}. However, it offers no facilities to take source-side context
information in account. We had to consider three options to achieve our goal of
integrating source-side context : 1) creating a new decoder; 2) enhancing
Moses; or 3) using a bypass method. We decided, in line with most of the
literature, to follow the third option, as it was the easiest and allows us to
use Moses as a black box.


immediately immediately draws our interest is $p(t|s)$. 

For the modelling source-side context the component of the score
vector that 

immediately immediately draws our interest is $p(t|s)$. 



Each of the components of the
score vector



The phrase-translation table embodies the translation model, and is 








Moses features, amongst others, a stack-based decoder. For each
decoding run, there are as many stacks as there are words in the source
sentence. Translation proceeds by translating 

charged with the tasked 

Integrating source-side context in 


\section{System}
\label{ref:system}








\end{document}
