%to be submitted to SoftwareX

\begin{document}


\section{Introduction}

The n-gram, a sequence of $n$ consecutive word tokens, is a core concept for
many Natural Language Processing (NLP) applications. One of the most NLP basic
tasks is to read corpus text and compute an $n$-gram frequency list, elementary
for any kind of statistical analysis. The unigram frequency list, i.e.\ word
frequency list, is the simplest instance of this task that is especially
ubiquitous. Computing $n$-gram frequency on a corpus text is fairly trivial,
and any beginning computer science student will have no trouble to accomplish
this in just a few lines of code in a modern high-level programming language.
Nevertheless, this computation is at the heart of Colibri Core, the NLP sofware
we aim to introduce in this paper, as there is more to it than meets the eye at
first.

N-grams are typically distributed in a Zipfian fashion, implying there are only
a few high-frequency patterns, with words such as common function words in the
lead, and there is a long tail of patterns that occur only very sparsely. This
basic fact makes counting a notoriously memory-hungry enterprise, as patterns
occurring below your minimum frequency treshold can not be discarded from
memory until the entire corpus has been processed. 

When working with large data sets and higher-order $n$-ngrams, this memory
problem becomes apparent quickly when trivial solutions are employed. Colibri
Core, on the other hand, offers tools and programming libraries that are
heavily optimised to 1) reduce memory usage, and 2) have high performance.

Colibri Core is developed in the scope of the ``Constructions as Linguistic
Bridges'' PhD study, which roughly focusses on the role of constructions, i.e.\ 
patterns such as $n$-grams, in their context, in Machine Translation. Machine
Translation, as well as Language Modelling are two common fields where large
data sets are often encountered, as more data generally improves the quality of
the resulting translation model or language model. 


The task of finding $n$-grams is generalised in Colibri Core to the task of
finding \emph{patterns} or \emph{constructions} (we use the terms
interchangeably). Furthermore, once patterns are identified, resulting in a
\emph{pattern model}, Colibri Core can extract relations between the patterns.

Colibri-core is a memory-based technique where models are held entirely in
memory to guarantee maximum performance on lookup and computation. It does
place considerable memory requirements on the system on which is it run. This
approach can be contrasted to e.g.\ database approaches which have much higher
latency and where the data consumes far more memory/diskspace.

\section{Patterns}

We distinguish three categories of patterns, and define them as follows:

\begin{enumerate}
    \item N-grams - A sequence of $n$ word tokens that are all consecutive.
    \item Skipgrams - A fixed-length sequence of $p$ word tokens and $q$ token placeholders/wildcards ($n=p+q$), the placeholders constitute gaps or skips and a skipgram can contain multiple of these. In turn, a gap can span one or more tokens. 
    \item Flexgrams - A sequence s, with one or more gaps of variable length, which implies the pattern by itself is of undefined length.
\end{enumerate}

Our definitions are defined narrowly and, with exception of $n-gram$ do not
necessarily correspond to the way the concepts are used in other studies. Some
may use the term skipgram to include what we call flexgram.  The term
``elastigram'' is sometimes also employed by others to refer to flexgrams. 
%TODO: cite something

As we deal with word tokens, this implies that the corpus data has to be in a
tokenised form. We start from the basis of plain-text corpus data, containing one
``unit'' per line; a unit can correspond to a sentence, paragraph, tweet
or whatever unit is deemed appropriate for the task at hand. Corpus data can
alternatively be provided in FoLiA XML \cite{TODO} as well, although linguistic
annotation will typically be ignored.

Text data is typically stored in a string of characters, the
characters themselves drawing their meaning from a character encoding. The
storage of a huge amount of strings is inefficient from a memory perspective,
considering the fact that words follow a Zipfian distribution. Colibri Core
therefore works on the basis of a lossless compression, in which each unique
word token is assigned a numeric class. This effectively defines the
``vocabulary'' of your data, and is called a \emph{class encoding}. A pattern
is then not represented as an array of characters, but as an array of these
classes instead. Further lossless compression is achieved by holding this array
of classes in a dynamic-length byte representation, in which low class values
can be stored in less bytes than high class values. Classes $1$ to $16$ can be
stored in a single byte, higher classes require at least two bytes. To achieve
maximum compression, classes are assigned to word tokens based on frequency:
words with the highest frequency receive the lowest classes.

The implementation's grammar confirms to the regular expression in
Equation~\ref{eq:patterngrammar}, where $p$ is a pattern, $l$ is a single-byte
length marker where $l<128$, $c$ is an $l$-byte sequence holding a class number
(big endian), and $m>=128$ is a special marker used to indicate placeholder
tokens for skipgrams $(128)$, or dynamic-width gaps for flexgrams
$(129)$\footnote{$m$ and $lc$ are mutually exclusive}.

\begin{equation}
\label{eq:patterngrammar}
p \leftarrow (lc|m)+
\end{equation}

To encode a text corpus, a class encoding needs to be computed first. To decode
the encoded corpus back to plain text, the class encoding is needed again.
Colibri Core provides tools and exposes library functions to do this.

\section{Informed Iterative Counting}

N-gram frequency lists are often parametrised by a certain threshold, $n$-grams
with less occurrences than this threshold are pruned. Because of the Zipfian
nature of our $n$-gram distribution, we can circumvent the problem of having to
hold a huge amount of patterns in memory which won't exceed our threshold. We do this by
employing informed counting. Informed counting is an iterative procedure, shown
in pseudo code in Algorithm~\ref{alg:ngramcounting}. Here we take $m$ to be the maximum
$n$-gram order we intend to extract, the whole corpus is then processed for
each $n$ where $1<n\leq\m$, extracting the respective $n$-grams at each
iteration. This means that at each iteration, we can consult the results of the
previous iteration. We can then readily discard any $n$-gram with $n>1$ for
which either of the two $n-1$-grams that it by definition contains does not
meet the threshold, as it follows that the $n$-gram can never meet the
threshold either. By outright discarding an $n$-gram we do not need to store it
and its count in memory. After each iteration of $n$, we prune all the
$n$-grams that did not reach the threshold.


\begin{algorithm} \caption{Informed Iterative Counting for n-grams.  Take $m$
to be the maximum $n$-gram order we intend to extract, $t$ to be the minimum occurrence threshold, and $M$ to be the
pattern model in memory, with unigrams already counted in the more trivial fashion.}
\label{alg:ngramcounting}
\begin{algorithmic}
\For {$n \in 2..m$}
    \For {$line \in corpus$}
        \For {$ngram \in extractngrams(line,n)$}
            \State  $nm1gram_1, nm1gram_2 \leftarrow extractngrams(ngram,n-1)$
            \If {$M(nm1gram_1) \geq t$ \And $M(nm1gram_2) \geq t$}
                \State $M(ngram) \leftarrow M(ngram) + 1$
            \EndIf
        \EndFor 
      \EndFor
  \EndFor
  \State $M \leftarrow prunengrams(M,n,t)$
\EndFor \\
\Return{M}
\end{algorithmic}
\end{algorithm}

Though not expressed in the simplified algorithm above, the actual
implementation accounts for more parameters, such as setting a lower bound to
$n$. The amount of back-off, going all the way up to $m-1$ here, can also be
fine-tuned.

\subsection{Informed Skipgram Counting}
\label{sec:skipgramcount}

The computation of skipgrams is parametrised by an upper limit $l\leq\m$ in the number of
tokens, as the possible configuration of gaps increases exponentially with the
total length spanned. It first requires a count of all $n$-grams where $0<n\geq\l$. 

The algorithm, shown in Algorithm\~ref{alg:skipgramcount} considers all
possible ways skips can be inserted in all of the n-grams in the model. It can
discard a skipgram candidate by looking at the non-skip parts that make up the
skipgram, and checking whether those exceed the set threshold. 

\begin{algorithm} \caption{Informed Counting for skipgrams.  Take $l$
to be the maximum skipgram order we intend to extract, $t$ to be the minimum occurrence threshold, and $M$ to be the
pattern model in memory, with ngrams already counted.}
\label{alg:skipgramcount}
\begin{algorithmic}
\For {$n \in 3..l$}
    \For {$ngram \in getngrams(M,n,t)$}
        \For {$skipgram \in possibleconfigurations(ngram)$}
            \State $docount \leftarrow \True$
            \For {$part \in parts(skipgram)$}
            \If {$M(part) < t$}
                    \State $docount \leftarrow \False$
                    \Break
                \EndIf
            \EndFor 
            \If {$docount$}
                \State $M(skipgram) \leftarrow M(skipgram) + 1$
            \EndIf
        \EndFor 
    \EndFor
  \EndFor
  \State $M \leftarrow pruneskipgrams(M,n,t)$
\EndFor \\
\Return{M}
\end{algorithmic}
\end{algorithm}

In this algorithm, the \texttt{possibleconfigurations(ngram)} function returns
all possible skipgram configurations for the given $n$-gram. Note that
configuration of gaps depends only on the length of the $n$-gram, and can
therefore easily be pre-computed. The \texttt{parts(skipgram)} function returns
all consecutive parts that are subsumed in the skipgram, i.e.\ the parts you
obtain when breaking at each skip.

Like Algorithm~\ref{alg:ngramcount}, Algorithm~\ref{alg:skipgramcount}
presupposes a threshold $t>1$. For threshold one, more trivial algorithms are
invoked, as the user does not want to prune anything. These make only a single
pass over the data. For skipgrams this leads to an explosion of
resulting patterns, exponential with number of tokens, and is best avoided.

\section{Pattern Models}

The pattern model is a $key\mapto\value$ store, where the keys correspond to
patterns. The underlying C++ library allows for choosing the actual underlying
container implementation through templating. The default container datatype is
a hash map\footnote{using the \texttt{unordered\_map} STL container in C++11},
which guarantee $O(1)$ access and update times under ideal hashing conditions. The
hash\footnote{Spooky Hash v2 is used for hashing:
http://burtleburtle.net/bob/hash/spooky.html} is computed directly from the
binary representation of a pattern.

In the underlying C++ implementation, the values of the pattern model are subject to
templating as well. We distinguish two types of pattern models:

\begin{enumerate}
 \item \textbf{Unindexed Pattern Models} -- Values are simple integers
 \item \textbf{Indexed Pattern Models} -- Values are arrays of indices where the pattern can be found in the corpus
\end{enumerate}

Obviously, indexed pattern models make a considerably higher demand on memory.
They do however, allow for a broader range of computations.

\section{Relations between Patterns}

Various relations can be extracted between patterns in a pattern model, for all
but the first of the relation types an indexed pattern model is required.

\begin{itemize}
 \item \textbf{Subsumption} -- $n$-gram $x y z$ subsumes $n-1$-grams $x y$ and $y z$. 
 \item \textbf{Successor} -- Patterns that are in a sequence in the corpus data. 
 \item \textbf{Instantiation Relations} -- Skipgrams or flexgrams may be
     \emph{instantiated} by other patterns. For example, ``to be \_ not \_'' be
     is instantiated by ``or \_ to'', resulting in the 6-gram ``to be or not to be''. This type of relation thus allows you to precisely determine what patterns occur in certain gaps.
 \item \textbf{Co-occurrence Relations} -- Different patterns can naturally co-occur multiple times
     within the the structural ``units'' you decided upon for the corpus (e.g.\ 
     sentences, paragraphs, tweets, etc). The measure of such co-occurrence 
     can be expressed by established metrics such as Jaccard and (normalised) mutual
     pointwise information.
\end{itemize}

For each of these categories, the relationship is bidirectional, i.e.\ you can
query for the subsuming patterns as well as the subsumed patterns, the left
neighbours as well as the right neighbours, the instantiations as well as the
abstractions. The co-occurrence relationship is fully symmetrical. These latter
three relationships rely on both the \emph{forward index} inherent in an
indexed pattern model, as well as the \emph{reverse index}, a function from
positions in the corpus to an array of patterns that are found at said
position, that can be automatically computed from the forward index. In the
implementation, the reverse index is not modelled in memory as a mapping from
positions to patterns, but as much less memory-consuming mapping from positions
to unigrams; the patterns can be derived on-the-fly from that.
%TODO

\section{Corpus Comparison}

The computation of pattern models on two or more distinct corpora, provided the
class encoding is the same for all of them, provides a basis for comparative
corpus analysis.

%TODO

\section{Software specifications}

Colibri Core consist of a C++ library, a Python binding\footnote{Both for
Python 2 as well as Python 3, written in Cython.} to said library and a set of
command-line tools. This allows for an interface to the sofware at various
levels, accommodating the need in different use cases and catering for
different levels of expertise. The available tools are:

\begin{itemize}
    \item colibri-patternmodeller - This is the core tool, it allows for the
        construction and inspection of pattern models.
    \item colibri-classencode - For the construction of a class encoding and
        the encoding of corpus data
    \item colibri-classdecode - For decoding encoded corpus data, given a class
        encoding
\end{itemize}

There are also higher-level shell scripts that invoke the above tools, so basic
tasks can be accomplished more easily without exposing the low-level functionality:

\begin{itemize}
    \item colibri-ngrams - Extracts n-grams of a particular size from the
        corpus text, in the order they occur, i.e.\ by moving a sliding window over the text.
    \item colibri-freqlist - Extracts all n-grams from one or more corpus text files and outputs a frequency list. Also allows for the extraction of skipgrams. By default all n-grams are extract, but an occurrence threshold can be set with the -t flag.
    \item colibri-ngramstats - Prints a summary report on the ngrams in one or more corpus text files. To get the full details on interpreting the output report, read the section Statistical Reports and Histograms.
    \item colibri-histogram - Prints a histogram of pattern occurrence count
    \item colibri-queryngrams - Interactive CLI tool allowing you to query ngrams from standard input, various statistics and relations can be outputted.
    \item colibri-reverseindex - Computes and prints a reverse index for the specified corpus text file. For each token position in the corpus, it will output what patterns are found there (i.e start at that very same position)
    \item colibri-loglikelihood - Computes the log-likelihood between patterns in two or more corpus text files, which allows users to determine what words or patterns are significantly more frequent in one corpus than the other.
    \item colibri-coverage - Computes overlap between a training corpus and a test corpus, produces coverage metrics.
\end{itemize}

The tools and scripts in Colibri Core are explicitly geared for command-line
usage. However, a RESTful webservice as well as generic web interface to
Colibri Core is also provided using CLAM\cite{CLAM}.

The software should compile and run on a wide variety of Unix-based systems,
including Linux, FreeBSD and Mac OS X. Everything is released under the GNU
Public License (v3)\footnote{TODO}.

Documentation, including API reference, is provided at
\url{http://proycon.github.io/colibri-core/doc/}. An extra tutorial for Python
is available at
\url{http://proycon.github.io/colibri-core/doc/colibricore-python-tutorial.html}.

\section{Benchmarks}

%TODO











\end{document}
