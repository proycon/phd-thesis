%LaTeX style for Journal of Open Research Software
\documentclass[a4paper,12pt]{article}

\usepackage{geometry}
\geometry{a4paper, total={210mm,297mm}, left=3.17cm, right=3.17cm, top=1.25cm, bottom=4cm }



\usepackage{hyperref}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{parskip}
\usepackage{sectsty}
\sectionfont{\fontsize{13}{13}\selectfont}
\subsectionfont{\fontsize{13}{13}\selectfont}
\subsubsectionfont{\fontsize{11}{11}\selectfont}
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\thesection}{(\arabic{section})}
\usepackage{titlesec}
\usepackage{scrextend}
\usepackage{natbib}
\bibpunct{[}{]}{;}{n}{,}{,}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{12pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

%Copyright Notice
%Authors who publish with this journal agree to the following terms:

%Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a Creative Commons Attribution License that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.

%Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.

%By submitting this paper you agree to the terms of this Copyright Notice, which will apply to this submission if and when it is published by this journal.


%Number only sections, not subsections
\setcounter{secnumdepth}{1}


\title{Efficient n-gram, skipgram and flexgram modelling with Colibri Core}

\begin{document}

\section{Overview}

\subsection{Title}
\textbf{Efficient n-gram, skipgram and flexgram modelling with Colibri Core}

\subsection{Paper Authors}
\begin{enumerate}
    \item van Gompel, Maarten;
    \item van den Bosch, Antal;
\end{enumerate}

\subsection{Paper Author Roles and Affiliations}
\begin{enumerate}
    \item PhD Candidate - Centre for Language Studies, Radboud University Nijmegen
    \item Professor - Centre for Language Studies, Radboud University Nijmegen
\end{enumerate}

\subsection{Abstract}

Counting n-grams lies at the core of any frequentist corpus analysis and is
often considered a trivial matter. Going beyond consecutive n-grams to patterns
such as skipgrams and flexgrams already increases the demand for better
solutions. The need to operate on big corpus data does so even more.  Lossless
compression and less trivial algorithms are needed to lower the memory demands,
yet retain good speed, and keep the task manageable. Colibri Core is software
for the efficient computation of n-grams, skipgrams and flexgrams from corpus
data. The resulting pattern models can be analyzed and compared in various
ways. The software offers a programming library for C++ and
Python, as well as command-line tools.

\subsection{Keywords}
Natural Language Processing; n-grams; skipgrams; corpus frequency; corpus analysis

\subsection{Introduction}

The n-gram, a sequence of $n$ consecutive word tokens, is a core concept for
many Natural Language Processing (NLP) applications. One of the most basic NLP
tasks is to read corpus text and compute an $n$-gram frequency list, elementary
for any kind of statistical analysis. The unigram frequency list, i.e.\ the word
frequency list, is the simplest instance of this task which is especially
ubiquitous. Computing $n$-gram frequency on a corpus text is fairly trivial,
and any beginning computer science student will have no trouble to accomplish
this in just a few lines of code in a modern high-level programming language.
Nevertheless, this computation is at the heart of Colibri Core, the NLP sofware
we aim to introduce in this paper, as there is more to it than meets the eye at
first.

N-grams are typically distributed in a Zipfian fashion, implying there are only
a few high-frequency patterns, with words such as common function words in the
lead, and there is a long tail of patterns that occur only very sparsely. This
basic fact makes counting a notoriously memory-hungry enterprise, as patterns
occurring below your minimum frequency treshold can not be discarded from
memory until the entire corpus has been processed. 

When working with large data sets and higher-order $n$-ngrams, this memory
problem becomes apparent quickly when trivial solutions are employed. Colibri
Core, on the other hand, offers tools and programming libraries that are
heavily optimised to 1) reduce memory usage, and 2) have high performance.

Colibri Core is developed in the scope of the ``Constructions as Linguistic
Bridges'' PhD study, which roughly focusses on the role of constructions, i.e.\ 
patterns such as $n$-grams, in their context, in Machine Translation. Machine
Translation, as well as Language Modelling are two common fields where large
data sets are often encountered, as more data generally improves the quality of
the resulting translation model or language model. 

The task of finding $n$-grams is generalised in Colibri Core to the task of
finding \emph{patterns} or \emph{constructions} (we use the terms
interchangeably). Furthermore, once patterns are identified, resulting in a
\emph{pattern model}, Colibri Core can extract relations between the patterns.

As the name Colibri Core suggests, the software is geared to provide
\emph{core} functionality for modelling patterns and exposes this functionality
as a programming library as well as through command line tools. It aims to
provide a solid foundation upon which more specialised software can be build,
such as software for language modelling. The software is aimed at
data scientists with a solid technical background and developers. 


\subsection{Implementation and Architecture}

\subsubsection{Patterns}

We distinguish three categories of patterns, and define them as follows:

\begin{enumerate}
    \item N-grams -- A sequence of $n$ word tokens that are all consecutive.
    \item Skipgrams -- A fixed-length sequence of $p$ word tokens and $q$ token placeholders/wildcards ($n=p+q$), the placeholders constitute gaps or skips and a skipgram can contain multiple of these. In turn, a gap can span one or more tokens. 
    \item Flexgrams -- A sequence $s$, with one or more gaps of variable length, which implies the pattern by itself is of undefined length.
\end{enumerate}

Our definitions are defined narrowly and, with exception of $n$-grams do not
necessarily match up precisely to the way the concepts are used in other studies. Some
may use the term skipgram to include what we call flexgram.  The term
``elastigram'' is sometimes also seen in the literature to refer to flexgrams. 

Skipgrams are used in the field to obtain a higher level of generalisation, which
can not be provided by n-grams. They can, for instance, be found as features in
classification tasks \cite{DHONDT}, or as a means in language modelling to
decrease perplexity \cite{Guthrie06}.

Dealing with word tokens implies that the corpus data has to be in a
tokenised form. We start from the basis of plain-text corpus data, containing one
``unit'' per line; a unit can correspond to a sentence, paragraph, tweet
or whatever unit is deemed appropriate for the task at hand. Corpus data can
alternatively be provided in FoLiA XML format \cite{FOLIAPAPER} as well, although linguistic
annotation will typically be ignored.

Text data is typically stored as a string of characters, the characters
themselves draw their meaning from a character encoding. The storage of a huge
amount of strings is inefficient from a memory perspective, considering the
fact that words follow a Zipfian distribution. Colibri Core therefore works on
the basis of a lossless compression, in which each unique word token is
assigned a numeric class. This effectively defines the ``vocabulary'' of your
data, which we call a \emph{class encoding}. A pattern is then not represented
as an array of characters, but as an array of these classes instead. Further
lossless compression is achieved by holding this array of classes in a
dynamic-length byte representation, in which low class values can be stored in
fewer bytes than high class values. Classes $1$ to $16$ can be stored in a
single byte, higher classes require at least two bytes. To achieve maximum
compression, classes are assigned to word tokens based on frequency (i.e.\
entropy encoding): words with the highest frequency receive the lowest classes.
This is essentially a variant of Huffman coding \cite{HUFFMAN}.

The implementation's grammar confirms to the regular expression in
Equation~\ref{eq:patterngrammar}, where $p$ is a pattern, $l$ is a single-byte
length marker where $l<128$, $c$ is an $l$-byte sequence holding a class number
(big endian), and $m>=128$ is a special marker used to indicate placeholder
tokens for skipgrams $(128)$, or dynamic-width gaps for flexgrams
$(129)$\footnote{$m$ and $lc$ are mutually exclusive}.

\begin{equation}
\label{eq:patterngrammar}
p \leftarrow (lc|m)+
\end{equation}

To encode a text corpus, a class encoding needs to be computed first. To decode
the encoded corpus back to plain text, the class encoding is needed again.
Colibri Core provides tools and exposes library functions to do this.

\subsubsection{Informed Iterative Counting}

N-gram frequency lists are often parametrised by a certain threshold, $n$-grams
with less occurrences than this threshold are pruned. Because of the Zipfian
nature of our $n$-gram distribution, we can circumvent the problem of having to
hold a huge amount of patterns in memory which won't exceed our threshold. We do this by
employing informed counting. Informed counting is an iterative procedure, shown
in pseudo code in Algorithm~\ref{alg:ngramcounting}. Here we take $m$ to be the maximum
$n$-gram order we intend to extract, the whole corpus is then processed for
each $n$ where $1<n\leq m$, extracting the respective $n$-grams at each
iteration. This means that at each iteration, we can consult the results of the
previous iteration. We can then readily discard any $n$-gram with $n>1$ for
which either of the two $n-1$-grams that it by definition contains does not
meet the threshold, as it follows that the $n$-gram can never meet the
threshold either. By outright discarding an $n$-gram we do not need to store it
and its count in memory. After each iteration of $n$, we prune all the
$n$-grams that did not reach the threshold.


\begin{algorithm} \caption{Informed Iterative Counting for n-grams.  Take $m$
to be the maximum $n$-gram order we intend to extract, $t$ to be the minimum occurrence threshold, and $M$ to be the
pattern model in memory, with unigrams already counted in the more trivial fashion.}
\label{alg:ngramcounting}
\begin{algorithmic}
\For {$n \in 2..m$}
    \For {$line \in corpus$}
        \For {$ngram \in extractngrams(line,n)$}
            \State  $nm1gram_1, nm1gram_2 \leftarrow extractngrams(ngram,n-1)$
            \If {$M(nm1gram_1) \geq t$ \& $M(nm1gram_2) \geq t$}
                \State $M(ngram) \leftarrow M(ngram) + 1$
            \EndIf
        \EndFor 
    \EndFor
    \State $M \leftarrow prunengrams(M,n,t)$
\EndFor \\
\Return{M}
\end{algorithmic}
\end{algorithm}

Though not expressed in the simplified algorithm above, the actual
implementation accounts for more parameters, such as setting a lower bound to
$n$. The amount of back-off, going all the way up to $m-1$ here, can also be
fine-tuned.

A performance evaluation of this algorithm will be addressed later in the
section on \emph{Quality Control}.

\subsubsection{Informed Skipgram Counting}
\label{sec:skipgramcount}

The computation of skipgrams is parametrised by an upper limit $l\leq m$ in the number of
tokens, as the possible configuration of gaps increases exponentially with the
total length spanned. It first requires a count of all $n$-grams where $0<n\geq l$. 

The algorithm, shown in Algorithm~\ref{alg:skipgramcount} considers all
possible ways skips can be inserted in all of the n-grams in the model. It can
discard a skipgram candidate by looking at the non-skip parts that make up the
skipgram, and checking whether those exceed the set threshold. 

\begin{algorithm} \caption{Informed Counting for skipgrams.  Take $l$
to be the maximum skipgram order we intend to extract, $t$ to be the minimum occurrence threshold, and $M$ to be the
pattern model in memory, with ngrams already counted.}
\label{alg:skipgramcount}
\begin{algorithmic}
\For {$n \in 3..l$}
    \For {$ngram \in getngrams(M,n,t)$}
        \For {$skipgram \in possibleconfigurations(ngram)$}
            \State $docount \leftarrow True$
            \For {$part \in parts(skipgram)$}
            \If {$M(part) < t$}
                    \State $docount \leftarrow False$
                    Break
                \EndIf
            \EndFor 
            \If {$docount$}
                \State $M(skipgram) \leftarrow M(skipgram) + 1$
            \EndIf
            \EndFor 
            \EndFor
    \State $M \leftarrow pruneskipgrams(M,n,t)$
\EndFor \\
\Return{M}
\end{algorithmic}
\end{algorithm}

In this algorithm, the \texttt{possibleconfigurations(ngram)} function returns
all possible skipgram configurations for the given $n$-gram. Note that
configuration of gaps depends only on the length of the $n$-gram, irregardless
of its content, and can therefore easily be pre-computed. The
\texttt{parts(skipgram)} function returns all consecutive parts that are
subsumed in the skipgram, i.e.\ the parts you obtain when breaking at each
skip.

Like Algorithm~\ref{alg:ngramcounting}, Algorithm~\ref{alg:skipgramcount}
presupposes a threshold $t>1$. For threshold one, more trivial algorithms are
invoked, as the user does not want to prune anything. These make only a single
pass over the data. For skipgrams this leads to an explosion of
resulting patterns, exponential with number of tokens, and is best avoided.

\subsubsection{What counts?}
\label{sec:whatcounts}

The counting algorithms are parametrised by a large number of parameters which
are not shown in Algorithm~\ref{alg:ngramcounting} and
Algorithm~\ref{alg:skipgramcount} to reduce complexity. The wide variety of
parameters allow the user to influence precisely what is counted and this is one
of the main assets of Colibri Core. Parameters exist to affect the following:

\begin{itemize}
    \item The minimum and maximum length (in words/tokens) of the n-grams
        and/or skipgrams to be extracted. Setting minimum and maximum length to
        the same value will produce a model of homogenous pattern length.
    \item A secondary word occurrence threshold can be configured. This is a value set higher than
        the primary occurrence threshold. Only patterns
        occuring above the primary threshold, and for which each of the
        individual words in the pattern passes the secondary threshold as well, will
        be included in the model.
    \item N-grams that are not subsumed by higher order n-grams, i.e.\ which do
        not  occur as part of a higher order n-gram in the data/model, can be pruned
        from the model. 
    \item Skipgrams can be constrained using the \emph{skip type threshold}. This
        requires that at least this number of distinct patterns must fit in any 
        skip in the skipgram. Higher values will produce less skipgrams, but
        typically more generic ones. 
    \item Skipgrams and n-grams are typically computed using the same
        occurrence threshold, but it is also possible to use a different threshold
        for skipgrams.
\end{itemize}

\subsubsection{Pattern Models}

The pattern model is a $key \mapsto value$ store, where the keys correspond to
patterns. The underlying C++ library allows for choosing the actual underlying
container implementation through templating. The default container datatype is
a hash map\footnote{using the \texttt{unordered\_map} STL container in C++11},
which guarantees $O(1)$ access and update times under ideal hashing conditions. The
hash\footnote{Spooky Hash v2 is used for hashing:
http://burtleburtle.net/bob/hash/spooky.html} is computed directly from the
binary representation of a pattern.

The use of hash maps can be contrasted to the use of suffix (or prefix) tries,
a common datastructure for storing n-grams. Although suffix tries benefit from
decreased memory use due to less overlap in pattern data. The strongly linked
nature of tries causes significant overhead in memory use\footnote{Each pointer
consuming 8 bytes on 64-bit architectures, and one would be needed between
every two tokens.} that well exceeds the memory footprint of hash maps. For
this reason, hashmaps are the default and tries are currently not implemented
in Colibi Core. The templating, however, easily allows for such an
implementation to be added in the future.

At this point, we need to briefly address suffix arrays \cite{Manber90} as well, which
are derived from suffix tries but with significantly decreased space
requirements. Suffix arrays with longest common prefix (LCP) arrays will
consume less memory than our hash maps, but are typically much slower to
construct and query. Though no exhaustive experiment was conducted to this
end, we did compare a predecessor of Colibri Core with a suffix array implementation
\cite{Stehouwer10} and found our implementation to be significantly faster in model
construction.

We distinguish two types of pattern model, depending on the type of the values,
which in the underlying c++ implementation is subject to templating as well:

\begin{enumerate}
 \item \textbf{Unindexed Pattern Models} -- Values are simple integers
 \item \textbf{Indexed Pattern Models} -- Values are arrays of indices where the pattern can be found in the corpus
\end{enumerate}

Obviously, indexed pattern models make a considerably higher demand on memory.
They do however, allow for a broader range of computations, as shall become
apparent in subsequent sections.

\subsubsection{Two-step training}

Training indexed patterns models is far more memory intensive than training
unindexed models, especially in huge corpora with a high type-token ratio. To
lower the demand on memory for such corpora, we implement a type of
\emph{two-step training}. This involves first constructing an unindexed pattern
model and subsequently constructing an indexed model on the basis of that, by
making another pass over the corpus and gathering all indices. The gain here is
in avoiding temporary storage of the indices that will not pass the occurrence
threshold but can not be ruled out a priori by the informed counting algorithm.
This conservation of memory comes at the cost of a extended execution time, so
it is up to the user whether to use this depending on his/her needs.

\subsubsection{Corpus Comparison}

The computation of pattern models on two or more distinct corpora, provided the
class encoding is the same for all of them, provides a basis for comparative
corpus analysis. One measure for corpus comparison introduced in the software
is the notion of ``coverage'', this metric is expressed as the number of tokens
in the test corpus that is covered by the patterns from the training corpus.
The metric can be represented either absolute, or in normalised form as a
fraction of the total amount of tokens in the test corpus. 

To perform such comparisons, we first compute a pattern model on the training
corpus, and subsequently compute a second pattern model on the test corpus, but
\emph{constrained} by the former pattern model. The ability to train
constrained models is present throughout the software and can for instance also
be used to train a pattern model based on a custom preset list of patterns. The
previously described two-step training algorithm is also an example of
constrained training.

Summarised statistics are computed at multiple levels. Measures such as
occurrence count and coverage can be consulted for aggregates of n-grams,
skipgrams, or flexgrams. The former two can be inspected specifically for each
of the different pattern sizes present in the model, i.e.\ for each value of $n$.

The coverage metric is a fairly crude metric of corpus overlap, despite the
ability to assess it for different aggregates. A more widely established metric
for corpus comparison is log-likelihood. Log likelihood expresses how much more
likely any given pattern is for either of the two models. It therefore allows
you to identify how indicative a pattern is for a particular corpus. Our
implementation follows the methodology of~\cite{Rayson00comparingcorpora}.

\subsubsection{Relations between Patterns}

Various relations can be extracted between patterns in a pattern model, for all
but the first of the relation types an indexed pattern model is required.

\begin{itemize}
 \item \textbf{Subsumption Relation} -- $n$-gram $x y z$ subsumes $n-1$-grams $x y$ and $y z$. 
 \item \textbf{Succession Relation} -- Patterns that are in a sequence in the corpus data. 
 \item \textbf{Instantiation Relation} -- Skipgrams or flexgrams may be
     \emph{instantiated} by other patterns. For example, ``to be \_ not \_'' be
     is instantiated by ``or \_ to'', resulting in the 6-gram ``to be or not to be''. This type of relation thus allows you to precisely determine what patterns occur in certain gaps.
 \item \textbf{Co-occurrence Relation} -- Different patterns can naturally co-occur multiple times
     within the the structural ``units'' you decided upon for the corpus (e.g.\ 
     sentences, paragraphs, tweets, etc). The measure of such co-occurrence 
     can be expressed by established metrics such as Jaccard and (normalised) mutual
     pointwise information.
\end{itemize}

For each of these categories, the relationship is bidirectional, i.e.\ you can
query for the subsuming patterns as well as the subsumed patterns, the left
neighbours as well as the right neighbours, the instantiations as well as the
abstractions. The co-occurrence relationship is fully symmetrical. 

These latter three relationships rely on both the \emph{forward index} inherent
in an indexed pattern model, as well as the \emph{reverse index}, a function
from positions in the corpus to an array of patterns that are found at said
position. This can be automatically computed from the forward index. In the
actual implementation, the reverse index is not modelled in memory as a mapping from
positions to patterns, but as a much less memory-consuming mapping from
positions to unigrams; the patterns can be derived on-the-fly from that.

\subsubsection{Flexgram Counting}

Thus-far, we have explained the algorithms for n-gram counting and skipgram
counting, but have not yet done so for flexgrams. The implementation allows
flexgrams to be computed in two different ways. The first is by simply
extracting skipgrams first, and then abstracting flexgrams from these
skipgrams. In this case the flexgram computation is constrained by the same
maximum-size limit the skipgrams are. The second method for flexgram extraction
proceeds through the co-occurence relation. A flexgram is formed whenever two
patterns (within the same structural unit) occur above a set threshold. The
implementation of this latter method is currently limited to flexgrams with a
single variable-width gap. 

\subsection{Quality Control}
\label{sec:qc}

To ensure the software is working as intended, a series of tests is available.
The tool \texttt{colibri-test} tests the various functions of the C++ library,
and the \texttt{test.py} script tests the Python binding. Testing in the form
of continuous integration is made possible through \emph{Travis CI}, where all
our test results can always be accessed
publicly.\footnote{\url{https://travis-ci.org/proycon/colibri-core}}

A performance evaluation of our iterative counting algorithm has been performed
by comparing our optimised C++ implemation with a naive implementation in
Python, as well as a simple implementation of informed iterative counting in
Python. To this end, we use a corpus of Dutch translations of TED talks of
$127,806$ sentences and $2,330,075$ words\footnote{The data is from the IWSLT
2012 Evaluation Campaign, \url{http://hltc.cs.ust.hk/iwslt/index.php/evaluation-campaign/ted-task.html\#MTtrack}.
Tokenisation was performed using ucto, \url{http://ilk.uvt.nl/ucto}.}. We set
the occurrence threshold to $2$ and extract everything from unigrams to
$8$-grams. The naive implementation, before pruning, consumes $2737$ MB and
runs in $23$ seconds\footnote{All these experiments were conducted using the
    included \texttt{benchmarks.py} script on a system
    with an Intel Core i7-4770K CPU \@ 3.50GHz, 16GB RAM, running Linux kernel
4.1}. A Python implementation of informed iterative counting significantly
lowers memory usage to $167$ MB, at the cost of increased CPU time; $65$
seconds. The Colibri Core implementation runs in $9.5$ seconds and takes memory
down to just $76$ MB, these gains can be attributed to the class encoding and
the fact that Colibri Core is implemented in C++.

Documentation, including API references for both Python and C++, is provided at
\url{http://proycon.github.io/colibri-core}. 

\section{Availability}

\subsection{Operation System}

Colibri Core should run on modern POSIX-compliant operating systems, including
Linux, FreeBSD and Mac OS X. It is tested to compile with current versions of
both \texttt{gcc} as well as \texttt{clang}.

\subsection{Programming Language}

Colibri Core is written in C++, adhering to the C++11 standard. The Python
binding is written in Cython (0.21 or above) and supports both Python 2.7 as
well as Python 3.

The Python binding ensures that the functionality of Colibri Core is available
from Python without sacrificing the great performance benefit native code
provides. Python was chosen as it is a high-level programming language in
widespread use in the scientific community, and the NLP community in
particular. It demands less expertise from the developer than C++ and is more
suitable for rapid prototyping.

\subsection{Additional system requirements}

Colibri Core provides memory-based techniques where models are held entirely in
memory to guarantee maximum performance on lookup and computation.  This
approach can be contrasted to e.g.\ database approaches which have much higher
latency.  It does place considerable memory requirements on the system on which
is it run, though this depends entirely on the size of the data and the
thresholds the user uses.  We recommend at least 16GB RAM, though using Colibri
Core on high-end computing servers with 256GB RAM is not uncommon for extensive
computation on big data sets.

Colibri-core is single-threaded due to the non-distributable nature of most of the
algorithms.

\subsection{Dependencies}

A standard C/C++ library and build environment; Python 2.7 or 3; Cython 0.21 or
above. Support for reading the \emph{FoLiA} format is optional and requires the
\texttt{libfolia} library.\footnote{https://proycon.github.io/folia}

\subsection{List of contributors}

Developed by Maarten van Gompel, with contributions and feedback from Louis
Onrust and prof. Antal van den Bosch. \emph{Centre for Language Studies, Radboud
University Nijmegen}.

\subsection{Software Location}

\subsubsection{Code repository}

\begin{addmargin}[2em]{2em}
\textbf{Name}: GitHub \\
\textbf{Identifier}: \url{https://github.com/proycon/colibri-core} \\
\textbf{Website}: \url{https://proycon.github.io/colibri-core} \\
\textbf{Licence}: GNU General Public Licence v3 \\
\textbf{Date published}: since September 21st, 2013 \\
\end{addmargin}

\subsection{Language}

English


\section{Reuse potential}

Colibri Core explicitly aims to provide a foundation for others in the NLP
community to build their tools and research on. The software is already being
employed in ongoing research on Machine
Translation\footnote{\url{https://github.com/proycon/colibri-mt}}, Bayesian Language
Modelling\footnote{\url{https://github.com/naiaden/cococpyp}}, spelling
correction\footnote{\url{https://github.com/proycon/gecco}}, and event
prediction\footnote{\url{https://github.com/fkunneman/ADNEXT\_predict}}.

As a programming library for both C++ and Python, Colibri Core can be
potentially adopted by a wide variety of 3rd party developers. As a set of
tools and scripts, Colibri Core also has merit by itself. It is, howver,
focussed on command-line usage and therefore still requires a certain technical
expertise from the end-user.

To increase the accessibility of Colibri Core, a RESTful webservice as well as
generic web interface is provided through CLAM\cite{CLAMPAPER}. With this we
hope to meet the needs of less technical end-users, as well as automated
networked clients. This webservice is hosted at
\url{https://webservices-lst.science.ru.nl}.

Future work building upon Colibri Core, may focus on offering more appealing
high-level interfaces to reach a wider audience.



\bibliographystyle{agsm}
\bibliography{coco_paper}


\end{document}
